\documentclass[12pt, a4paper]{report}
\special{papersize=210mm, 297mm}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[left=2.5cm, right=2.5cm, top=2.5cm]{geometry}
\renewcommand{\baselinestretch}{1.4}
\usepackage[toc,page]{appendix}
\pagenumbering{alph}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\graphicspath{{images/} {diagrams/}}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage[linesnumbered,ruled]{algorithm2e}

%\usepackage{showframe}
\usepackage{fullpage}

\usepackage{url}
%\usepackage{natbib} % for author-date citation \citep{}, \citet[]
%\usepackage{hyperref}
%\usepackage[nottoc]{tocbibind}
\usepackage{listings}
\usepackage{verbatim}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
	language=java,
	aboveskip=5mm, belowskip=3mm, showstringspaces=false,
	columns=flexible, basicstyle={\small\ttfamily},
	numbers=none, numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=3
}


\usepackage{multirow}
\usepackage{array}
\newcolumntype{L}[1]{> {\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\pagenumbering{arabic}

\begin{document}
	
	%======================================================================
	
	\begin{titlepage}
		
		\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
		
		\center
		
		\vspace{-20pt}
		\includegraphics[width=100pt]{../images/FMI-03.png}\\[1.0cm]
		
		\textsc{\LARGE West University of Timisoara}\\[0.5cm]
		\textsc{\Large Faculty of Mathematics and Computer Science}\\[0.5cm]
		\textsc{\large Study Program: \\Computer Science in English}\\[3cm]
		\textsc{\Huge Master Dissertation}\\[5cm]
		
		\begin{minipage}{0.4\textwidth}
			\begin{flushleft} \large
				\textbf{COORDINATOR:}\\
				Associate Prof. Marc Eduard \textsc{Frîncu}
			\end{flushleft}
		\end{minipage}
		~
		\begin{minipage}{0.4\textwidth}
			\begin{flushright} \large
				\textbf{GRADUATE:} \\
				Maria Minerva \textsc{Vonica}
			\end{flushright}
		\end{minipage}\\[0.5cm]
			
		\vfill
		{\large Timi\c{s}oara \\2021}\\
		\vfill
		
	\end{titlepage}
	
	% =====================================================================
	
	\begin{titlepage}
		
		\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
		
		\center
		
		\textsc{}\\[.7cm]
		
		\textsc{\LARGE West University of  Timi\c{s}oara}\\[0.5cm]
		\textsc{\Large Faculty of Mathematics and Computer Science}\\[0.5cm]
		\textsc{\large Study Program: \\Computer Science in English}\\[4.5cm]
		
		\textsc{\Huge Master Dissertation}\\[2cm]
		
		{\Huge \bfseries {A Computer Vision Application for Predicting Satellite Imagery Focusing on Glacier Evolution}}\\[6cm]
		
		\begin{minipage}{0.4\textwidth}
			\begin{flushleft} \large
				\textbf{COORDINATOR:}\\
				Associate Prof. Marc Eduard \textsc{Frîncu}
			\end{flushleft}
		\end{minipage}
		~
		\begin{minipage}{0.4\textwidth}
			\begin{flushright} \large
				\textbf{GRADUATE:} \\
				Maria-Minerva \textsc{Vonica}
			\end{flushright}
		\end{minipage}\\[0.5cm]
		\vfill
		{\large Timi\c{s}oara\\ 2021}\\
		
		\vfill
		
	\end{titlepage}

	% =====================================================================
	
	\begin{abstract} %begin abstract  
		%The abstract should have one page and should be a compact presentation of the dissertation.
		\vspace{1.0cm}
		\vspace{1.0cm}
		\par Over the last decade, climate change has impacted Earths' atmosphere and environment more than anytime before, glaciers being the most sensitive indicators. In this work we are going to analyse the retreat of glaciers over time by using advanced computer vision algorithms on aerial imagery collected from the Landsat 8 satellite. Our aim is to detect movement based on dense optical flow generation over a time series of images and use the results for generating new ones in the series. Our results show that we were able to get relevant information by extracting motion tendencies across time and we have successfully generated new snapshots based on these. 
		
	\end{abstract} %end abstract

	\newpage{}

	\tableofcontents{}
	\addcontentsline{toc}{chapter}{List Of Figures}
	\listoffigures{}
	\addcontentsline{toc}{chapter}{List Of Tables}
	\listoftables{}
	
	\newpage{}

	\chapter{Introduction}
	
	\par According to a report delivered by NASA in March, 2021, Earth's average surface temperature which was recorded in 2020 came to be equal with the temperatures which designated 2016 as the hottest year on record \cite{NASA2020} and they are expecting that records will continue to be broken.
	
	\par Analysing temperature trends on a global scale provides vital indicators such as carbon dioxide levels in the atmosphere, which are now growing more than they can be naturally eliminated, causing an increase in  phenomena such as sea ice and ice sheet mass loss, heat waves which are will become more intense and longer, rising level of the sea, as well as changes in the fauna and flora of the planet.
	
	\par Having a mature enough approach for timely identification of these climate trends represents an essential necessity for human life. Changes in environment will require changes of approach to problems such as managing water resources, creating different crops which can withstand extreme changes of temperature and being prepared for potentially disastrous weather events.
	
	\par Since glaciers are known the be the most sensitive indicators to climate change, extraction of information about their changes in the last years could prove valuable. Glacier retreat is happening due increased values in temperature, thus causing less snow fall and less accumulation of ice in time.
	
	\section{Motivation and goals}
	
	\par The first Earth observation satellite was Vanguard 2, designed to collect satellite imagery in a consistent and long-term manner. NASA followed with Landsat 1, launched in 1972, until Landsat 8 (2013) and Landsat 9, which will be functional starting with September 2021. The assets collected by the Landsat missions consist of millions of images of the Earth over the last almost 50 years and they can be further used in order to extract information on weather behaviour, ocean temperatures, vegetation health, droughts and various other environmental variables. There are applications which extract patterns based on time-series analysis in order to understand why certain phenomena happened in the past and what are the conditions required for them to reappear in the future as well. The Landsat archives contain already structured data, organized on image quality and acquisition dates. As an addition to this, over the last years we have developed more powerful tools which, due to increased storage and computational power, are able to take on such demanding applications.

	\par Having access to such a valuable datasets, we could turn to confidently analyse changes in glacier retreat and snow fall tendencies over the last 8 years, focusing on Landsat 8 imagery.  The goal of this thesis is to make use of such data by analysing changes of glaciers in the last decade with the purpose of creating predicted images which could highlight future glacier retreat or different patterns of accumulation. Creating support for easy asset acquisition of satellite imagery is also important, since current methods do not scale for larger dataset acquisition. 

	\section{Related work}
	
	\par In \cite{WINSVOLD2016}, the authors use both Sentinel and Landsat satellites for image collection in order to produce a higher quantity of qualitative images for glacier mapping, while also covering wider areas. They propose four approaches for studying automatic glacier mapping. The first is by analysing different band ratios for multispectral image creation while the second proposes robust methods used for improving glacier mapping by exploitation of seasonal variation yielded by the spectral properties of snow. The third one highlights spatio-temporal variation of glacier surface types and the fourth one analyses how the chosen band ratio images generated from the first application can be used for automatically detecting changes in glaciers. Their results show that the alpine and arctic advances and retreats provide the most visible signs of climate change and they propose to revisit current methods for glacier mapping in order to reduce the practice of manual glacier mapping.
	
	\par Another paper which studies glacier sensitivity to climate change is \cite{TAK2020}, in which the authors propose a remote sensing based framework used on multispectral satellite imagery, used for studying the ablation and accumulation processes of glaciers in order to quantify their mass balance. They have studied the mass loss during of the Parvati glacier, located in the western Himalaya, between 1998 and 2016. Their results show that the glacier responds to climate change factors each year through a high mass loss, resulting in a strong effect on water availability and river flows.
	
	\par In \cite{RACOVITEANU2019}, the authors propose a decision-based image classification algorithm for separating ice, debris and snow surfaces on glaciers and extracting snow lines both monthly and annually. They achieved this by automatically partitioning glacier surfaces through band ratios combined with topographic criteria which were extracted for each pixel. They have conducted the study on Karakoram and Trishuli regions located in eastern Himalaya, with images taken from 2000 to 2016 and they have concluded that snow lines were the most sensitive to manual corrections, elevation dataset, topographic slope and the calculated thresholds for the band ratios for the spring and winter months.
	
	\par In \cite{WINSVOLD2017}, the authors propose descriptive methods for glacier mapping by using aerial time series produced by the Synthetic Aperture Radar sensor from Radarsat-2 and Sentinel-1A. Out of their five scenarios, the first tracks transient snow lines and it proved to correlate with both Landsat 8 and Sentinel-2A produced aerial images. As a conclusion, they state that automatically derived satellite imagery products prove to be important in analysing glacier change.
	
	\par Another interesting approach to determine glacier melt was analysed in \cite{WANG2017}, where the authors have compared the band ratio method with the Normalized Difference Snow Index (NDSI) one for extracting parameters which highlight glaciers. They have conducted their studies on the Karakorum area with satellite imagery collected from Landsat 8, starting from 2014. Their results show that for boundary extraction, the band ratio technique yielded better results. They also state that visual interpretation is still an major factor in analysing the obtained results.
	
	\par As for motion estimation, Farneb{\"a}ck, Gunnar proposes an interesting approach in \cite{GUNNAR2003}. The author presents an algorithm for estimating motion between two frames by approximating each neighbourhood of both by quadratic polynomials. Their transformation under translation is then analysed in order to extract displacement fields.

	\section{Our Contribution}
	
	\par This thesis makes use of techniques such as computer vision and machine learning for generating predicted images based on detected motion. This can be used to highlight glacier retreat over longer periods of time. For this purpose we had to implement techniques to align our images with respect to each other, as well as determining ways to make use of the extracted optical flow information, based on which we generate new images.

	\par For the alignment process, more than 200 pairs of images have been tested, while only around 10\% of them having unsatisfactory alignment results. As for the image generation, since the generated images rely mostly on visual validation, we have compared the ice and snow coverage on multiple datasets with different coordinates. In order to better enhance the differences in snow retreat and accumulation which are predicted by the motion generated image, we have colourized them. The results can be seen in the Section~\ref{seq:experiments}. 
	
	\par The results were achieved through indexing glacier datasets by a crawler, which has the responsibility of structuring the data into path and row coordinates (see Paragraph~\ref{par:wrs2}), such that we are working with geographically consistent data. The normalized snow difference index has been calculated as described in Section~\ref{seq:ndsi_functional}, by using two Landsat 8 bands. We have chosen this metric to enhance snowfall and ice due to their high reflectance in the visible spectrum and high absorption in the infrared one.
	
	\par Our goal is to process these images as pairs, so we had to take into consideration the fact that Landsat 8 does not have a perfectly stable trajectory, which yields in slightly misaligned pictures. However, this discrepancy would prove to generate unreliable results in the predicted image, therefore multiple techniques of solving this problem have been tested. Details of these can be found at Section~\ref{seq:alignment_functional}.

	\par For the image generation we used a computer vision algorithm which calculated the optical flow between two consecutive frames, which resulted in a matrix holding the distance travelled by each pixel from one frame to another. Based on this distance, new coordinates have been predicted for the pixels and they have been moved accordingly, resulting in a new image representing the change over time. Various methods of filtering have been applied on the results, as it is described in Section~\ref{seq:motion_ndsi_functional}. In order to validate our results, we have computed predicted images for all the pairs of the dataset, and we have compared them to already existing consecutive ones. The snow and ice coverage of the motion generated image can be then compared with its neighbours.

	\par Since our application focuses on working with images, a graphical user interface seemed fit for this purpose. However, for the search and download part, a simple command line script has been used (see Section~\ref{seq:gui}).
	
	\newpage{}
	
	\chapter{Application}
	\section{Dataset}

	\par In order to build the dataset, we made use of the freely available data from the Landsat Archive, specifically from collection 1, level 1. This dataset contains assets which are generated from the Landsat 8 Sensors, as well as entries from other older Landsat satellites. For the purpose of this paper, we will focus only on images collected from the Landsat 8 satellite.
	
	\subsection{Landsat 8}
	\label{seq:landsat8_section}

	\par We will use images collected by the Landsat 8 satellite (Figure~\ref{fig:landsat_satellite}), which was the most recently lauched on the Atlas-V rocket (Vandenberg Air Force Base), in California on February 11, 2013. For remote sensing, Landsat 8 is equipped with two sensors, the Thermal Infrared Sensor (TIRS)  and Operational Land Imager (OLI) one.
	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.25]{../images/LandsatSatellite.png}
		\caption{Landsat 8 satellite \cite{LANDSATPIC}.}
		\label{fig:landsat_satellite}
	\end{figure}

	\par The satellite is orbiting the Earth at an altitude of 705 km, completing one orbit every 99 minutes. Based on this trajectory, the satellite has a 16 day repeat cycle and it acquires 740 scenes each day. These are organised on the path/row system defined by Worldwide Reference System-2. A Landsat 8 scene size is 185 km x 180 km \cite{LANDSAT}. 
	
	\paragraph{Worldwide Reference System-2}
	\label{par:wrs2}
	
	\par We will be referring to each scene's location based on its path and row coordinates from the WRS-2, which is a notation system used for Landsat images. This system is used with the main goal of keeping a structured archive of Landsat data which can be easily catalogued and accesses. Users can access aerial imagery through querying the specified path and row variables \cite{wrs}. Landsat's trajectory projected on the world map can be seen in Figure~\ref{fig:wrs2}.

	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.2]{../images/wrs2.png}
		\caption{WRS-2 Path/Row for Landsat \cite{wrs}.}
		\label{fig:wrs2}
	\end{figure}
	
	\subsubsection{Landsat scene naming convention}
	\label{seq:landsat_naming}
	
	\par Each Landsat scene is named after a well-defined convention in order to easily check information such as the WRS-2 path, row and the date of acquisition. Having access to this information without the need to download the scene itself or the metadata file which holds this information represents a valuable asset, since we can easily filter the data based on the naming convention itself. In the table \ref{table:landsat_table} below we represent what each part of a Landsat scene of the form \textbf{LXS PPPRRR YYYYDDD GSIVV} means.
	\begin{table} [h!]
		\center
		\begin{tabularx}{480pt}{|X|X|}
			\toprule
			\textbf{L} & Landsat \\ [0.2ex]
			\midrule
			\textbf{X} & Sensor \\ [0.2ex]
			\midrule
			\textbf{SS} & Satellite \\ [0.2ex]
			\midrule
			\textbf{PPP} & WRS path \\ [0.2ex]
			\midrule
			\textbf{RRR} & WRS row \\ [0.2ex]
			\midrule
			\textbf{YYYY} & Acquisition year \\ [0.2ex]
			\midrule
			\textbf{DDD} & Julian day of the acquisition year \\ [0.2ex]
			\midrule
			\textbf{GSI} & Ground station identifier \\ [0.2ex]
			\midrule
			\textbf{VV} & Archive version number \\ [0.2ex]
			\midrule
			\midrule
			\bottomrule
		\end{tabularx}
		\caption{Landsat 8 scene naming convention \cite{sn}.}
		\label{table:landsat_table}
	\end{table}
	
	\subsubsection{Operational Land Imager}
	
	\par The Operational Land Imager (OLI) represents remote sensing instrument which can be found abroad Landsat 8. It is built by Ball Aerospace \& Technologies and it collects moderate resolution data which is used for monitoring changes in trends on the surface and evaluating how it changes over time. The images and data which OLI has helped collect have practical applications today in various fields such as mapping and monitoring changes in snow, ice, water and agriculture, \cite{KNIGHT2014}. 
	The OLI operates in the short wave infrared spectral and visible region with a width of 185 km. Wavelengths of 443 nm to 2,200 nm are translated into nine channels, from which  eight are multispectral and one is panchromatic. The eight multispectral ones have a 30-meter spatial resolution, while the panchromatic channel has a 15 meters one.
	The OLI generates 9 bands for Landsat as shown in Figure~\ref{fig:L8OLI}.
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.5]{../images/Landsat8-OLI-Bands.png}
		\caption{Landsat 8 OLI generated bands \cite{l8otb}}
		\label{fig:L8OLI}
	\end{figure}
	
	\subsubsection{Thermal Infrared Sensor}
	
	\par The Thermal Infrared Sensor (TIRS) measures land surface temperature in two thermal bands with a new technology that uses quantum mechanic techniques in order to detect thermal infrared wavelengths of light which are emitted by the Earth \cite{lgng}. The thermal infrared sensor generates 2 bands for Landsat as shown in Figure~\ref{fig:L8TIRS}.
	
	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.5]{../images/Landsat8-TIRS-Bands.png}
		\caption{Landsat 8 TIRS generated bands \cite{l8otb}}
		\label{fig:L8TIRS}
	\end{figure}
	
	\subsection{World Glacier Inventory}
	\label{seq:wgi}
	
	\par The World Glacier Inventory (WGI) proves to be a useful resource for building our dataset, since it contains over 130,000 entries representing glaciers. Various parameters are stored in this file for each glacier, such as its geographic location in the form of latitude and longitude coordinates, total area, its elevation, orientation and many more. The dataset has been constructed through aerial satellite mapping,therefore the set can be viewed as a snapshot of the glacier distribution from 2012 \cite{WGI}.  
	
	\par There are a number of ways to retrieve data from the inventory:
	\begin{itemize}
		\item download the dataset in a single CSV file (wgi\_feb2012.csv);
		\item search by parameter using the Search Inventory interface;
		\item extract regions through the Extract Selected Regions interface.
	\end{itemize}

	\par The CSV text file will be used with the purpose to define which are the glaciers to be included in the dataset to be built. An example of how this file looks like can be found in Figure~\ref{fig:WGI_ASCII}.
	
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.5]{../images/wgi_ASCII_file.png}
		\caption{World glacier inventory ASCII text file, as CSV}
		\label{fig:WGI_ASCII}
	\end{figure}
	
	\par The parameters which will be extracted for the dataset construction are the following:
	\begin{itemize}
		\item \textbf{wgi\_glacier\_id}: unique id representing one glacier (or part of it, if the coverage area is larger);
		\item \textbf{glacier\_name}: name of the glacier (if it has one);
		\item \textbf{lat}: latitude of the glacier;
		\item \textbf{lon}: longitude of the glacier.
	\end{itemize}

	\subsection{Asset Acquisition}
	
	\par Since Landsat 8 acquires over 700 scenes per day, this means that there are over two million scenes available for download, either making use of already built user friendly tools or by simply querying for them directly.
	
	\subsubsection{USGS Earth Explorer}
	
	\par One of the most popular services for satellite imagery downloading is USGS Earth Explorer. This is used for querying and ordering of satellite images, aerial photographs, and cartographic products through the U.S. Geological Survey. The tool is particularly useful when the main focus is to analyse a specific area rather than trying to acquire a large dataset of scenes. One can easily search for assets based on criteria such as world reference system path and row variables, latitude, longitude, cloud coverage, capture date and many others \cite{USGS}.
	
	\par However, downloading a large set of assets proves to be rather difficult by using this tool alone, since the parameters for each scene need to be manually set. On top of this, the query results have to be picked by hand and then passed for downloading through another application which handles their bulk download. This makes the process of building the dataset rather slow, frustrating and error prone. Such an example can be viewed in Figure~\ref{fig:EarthExplorer}, for the Belvedere glacier (45.942, 7.908).
	
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.25]{../images/EarthExplorer.png}
		\caption{EarthExplorer}
		\label{fig:EarthExplorer}
	\end{figure}
	
	\subsubsection{SpatioTemporal Asset Catalog API}
	\label{seq:STAC}
	
	\par In order to fix the problem of excessive manual labour which appeared by using the USGS Earth Explorer, we rather implemented an endpoint of the SpatioTemporal Asset Catalog API, specifically, the following: \textbf{\url{http://nsidc.org/data/glacier_inventory/index.html}} \cite{STAC}. The main idea of searching by using parameters still remains, but instead of manual inputting data for the search data, we rely on using the above-mentioned World Glacier Inventory ASCII text file, since it already has all the required information for each glacier.
	
	\par By using this method we can pick which glaciers we want to download based on their coordinates and calculate a bounding box representing the area we want to search, required for the STAC API query. Since there might be clouds which could obfuscate the area of interest in the image, we also add a maximum allowed cloud coverage along the bounding box.
	
	\par The STAC API query also requires a name for the collection of assets we want our queries to be made on, which for us is landsat-8-l1 (Landsat 8 Collection 1, Level 1). Using these three parameters we can now easily acquire a large number of assets with minimal manual labour, as compared to the more user friendly tool provided by USGS.
	
	\par The downloaded assets will be stored at a user specified disk location and they will be structured as shown in the Figure~\ref{fig:DownloadDirectory}.
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.4]{../images/DownloadDirectory.png}
		\caption{Download Directory}
		\label{fig:DownloadDirectory}
	\end{figure}
	
	\subsubsection{Landsat 8 Collection 1, Tier 1}
	
	\par To support analysis  of the Landsat long-term data record, the Landsat archive was restructured into a more formal data collection structure with the aim of keeping consistent information for each level 1 product. By making sure that these standards are met, the library can be used for various applications such as data stacking and time-series analysis \cite{lc1l1}. By using data from this collection, we ensure that our images are fit for accurate pixel-to-pixel processing.
	
	\section{Dataset entities}
	
	\par We will further describe which are the bands necessary for the image generation and what are their uses. On top of this, we will further explain what is the normalized snow difference index and what is the optical flow used for.
	
	\subsection{Bands}
	
	\par Each Landsat 8 band is represented by a 16 bit grayscale image with a resolution between 7000 and 10000 pixels, each pixel representing 30 meters. We can conclude, therefore, that one scene covers around 200 and 300 km of Earth. Only the green and SWIR1 bands will be used for the purpose of this thesis and below we will discuss the specifications of each. 
	
	\subsubsection{Band 3 - Green Band}
	
	\begin{table} [h]
		\center
		\begin{tabular} {| l | l |}
			\hline
			\textbf{Wavelength} & {0.53- 0.59 micrometers} \\ [0.2ex]
			\hline
			\textbf{Spacial resolution} & {30 meters} \\ [0.2ex]
			\hline
			\textbf{Resolution} & {between 7000x7000 pixels and 10000x10000 pixels} \\ [0.2ex]
			\hline
			\textbf{Depth} & {16-bit}\\ [0.2ex]
			\hline
			\textbf{Format} & {grayscale}\\ [0.2ex]
			\hline
		\end{tabular}
		\caption{Landsat 8 green band specifications.}
		\label{table:green_table}
	\end{table}
	\par The green band, alongside with the red and blue ones, fall in the visible spectrum and it is usually used for mapping peak vegetation. Figure~\ref{fig:green} is an example of the green band for the LC81950282015098LGN01 scene.
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.3]{../images/LC81950282015098LGN01_B3.png}
		\caption{Green band of scene LC81950282015098LGN01.}
		\label{fig:green}
	\end{figure}
	
	\subsubsection{Band 6 - SWIR1 Band}
	
	\begin{table} [h]
		\center
		\begin{tabular} {| l | l |}
			\hline
			\textbf{Wavelength} & {1.57 - 1.65 micrometers} \\ [0.2ex]
			\hline
			\textbf{Spacial resolution} & {30 meters} \\ [0.2ex]
			\hline
			\textbf{Resolution} & {between 7000x7000 pixels and 10000x10000 pixels} \\ [0.2ex]
			\hline
			\textbf{Depth} & {16-bit}\\ [0.2ex]
			\hline
			\textbf{Format} & {grayscale}\\ [0.2ex]
			\hline
		\end{tabular}
		\caption{Landsat 8 SWIR1 band specifications.}
		\label{table:swir1_table}
	\end{table}
	\par The shortwave infrared 1 band is particularly useful for enhancing object which look similar in other bands, such as soils and rocks \cite{bd}. Alongside this, it also discriminates moisture content of soil and vegetation and penetrates thin clouds \cite{bands}. Figure~\ref{fig:swir1} is illustrated below as an example of a SWIR1 band taken from the same scene as the green one above.
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.3]{../images/LC81950282015098LGN01_B6.png}
		\caption{SWIR1 band of scene LC81950282015098LGN01.}
		\label{fig:swir1}
	\end{figure}
	
	
	\subsection{Normalized Snow Difference Index}
	\label{seq:ndsi_functional}
	
	\par The normalized snow difference index (NDSI) is an index which relates to the presence of snow/ice in a pixel. Snow and ice usually have a very low reflectance in the shortwave infrared spectrum and very high in the visible one, which is useful for mapping out most types of clouds from the scene \cite{ndsi}. We can therefore use the formula from Equation~\ref{eq:ndsi_formula} in order to highlight the snow and ice pixels from a Landsat 8 image.
	
	\begin{equation}\label{eq:ndsi_formula}
		NDSI = \frac{green - swir1}{green + swir1}
	\end{equation}

	\par We can then apply the thresholding function from Equation~\ref{eq:ndsi_threshold} in order to separate snow free land from covered one. We have tried different values for the threshold, as proposed by Hall and Riggs in \cite{Hall2001}, finding that 0.3 yields the best results.
	
	\begin{equation}
		thresholding(pixel) = \left.
		\begin{cases}\label{eq:ndsi_threshold}
			\text{snow/ice},  & NDSI \geq 0.3\\
			\text{snow-free}, & NDSI < 0.3
		\end{cases}
		\right\}
	\end{equation}

	\par For easier visual analysis, we have coloured the result such that pixels which are considered to represent snow are white, while snow free land is represented with green. The resulting NDSI image can be viewed in Figure~\ref{fig:ndsi}.
	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=\columnwidth]{../images/LC81950282014271_ndsi.png}
		\caption{NDSI image of scene LC81950282014271, Jungfrau-Aletsch-Bietschhorn glacier.}
		\label{fig:ndsi}
	\end{figure}

	% ==================================================================================

	\section{Alignment}
	\label{seq:alignment_functional}
	
	\par Landsat’s trajectory orbiting Earth is not precise, therefore not all images will be
	aligned pixel-to-pixel. The green and swir1 bands have a spacial resolution of 30 meters, therefore a misalignment of just 50 pixels
	we would end up with a 1.5 km difference between two scenes. Tracking pixel motions through
	a series of images without aligning them first would yield in erroneous results, due to inconsistent geographical coordinates. Figure~\ref{fig:alignment_example} highlights the misalignment between
	scenes LC81950282013316 and LC81950282013364.
	
	\begin{figure}
		\centering
		\begin{minipage}[0.2]{\columnwidth}
			\centering
			\includegraphics[width=\columnwidth]{../images/LC81940282013213LGN01_LC81940282015219LGN01_unaligned.png}
		\end{minipage}
		\begin{minipage}[0.2]{\columnwidth}
			\centering
			\includegraphics[width=\columnwidth]{../images/LC81940282014216LGN01_LC81940282015219LGN01_aligned.png}
		\end{minipage}
		\caption{Overlapped unaligned (top) and aligned (bottom) NDSI scenes LC81940282014216LGN01 and LC81940282013213LGN01.}
		\label{fig:alignment_example}
	\end{figure}

	\par In order to solve this problem, we have used an algorithm which \textbf{collects features} from each band of a scene and tries to match them with a given reference one's features. The obtained \textbf{matches} can be then used to create an \textbf{affine transformation matrix} which specifies by how much did the current image \textbf{rotated and translated} in comparison to its reference. The affine transformation matrix will be then applied to the current image by a \textbf{warping} algorithm with the goal of ensuring as much as possible that there is no misalignment between any pixel of two different images from the same time series. Section~\ref{seq:alignment_algorithm} describes in a technical manner the approaches used to solve this and what were the computer vision algorithms used for that matter. 

	\subsection{Movement extraction through optical flow}
	\label{seq:ndsi_motion_matrix}
	
	\par Since our dataset can be viewed as a time series of satellite images, we propose that in order to generate a new image of this series, we should \textbf{track the motion} of each pixel from one frame to another.
	
	\par Extracting the motion vectors between two consecutive frames can be achieved by
	calculating their optical flow. Optical flow is defined as the motion of objects between
	consecutive frames of a series, produced by the relative movement between the object
	and camera. By using computer vision algorithms which calculate the optical flow of two
	scenes, we could track the changes in glacier retreat across a time series dataset such that we can estimate
	their current velocity and predict their position in the next frames.
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=\columnwidth]{../images/optical_flow_schema.png}
		\caption{Optical flow between NDSI(\(time=t\)) and NDSI(\(time=t+dt\)).}
		\label{fig:optical_flow_schema}
	\end{figure}
	
	\par Figure~\ref{fig:optical_flow_schema} emphasizes the problem visually, where we can express an image as a
	function of space, with the coordinates \((x, y)\), and time \(t\). If we take the first image
	to be \(I(x, y, t)\) and we move its pixels by a distance of \((dx, dy)\) over a timestamp \((dt)\), we obtain
	the new image as follows: \(I(x + dx, y + dy, t + dt)\).
	
	\par There are multiple types of optical flow generation algorithms, but for the purpose of our thesis,
	we have chosen a \textbf{dense optical flow} algorithm proposed by Farneb{\"a}ck in \cite{GUNNAR2003}. Even
	if dense implementations have a higher cost, we chose to make this trade mainly because it
	calculates the motion for each pixel of the frame while also having a higher accuracy compared to sparse methods such as in the one proposed by Lucas and Kanade in \cite{KANADE1981}.
	
	\par The generated results will be the computed motions for each pixel, treated as a pair representing the distance and direction that its coordinates moved from one frame to another. As referring to Figure~\ref{fig:optical_flow_schema}, each item from the result represents the motion distance vector (\(dx, dy\)) as calculated between NDSI(\(time=t\)) and NDSI(\(time=t+dt\)). As an example, the generated optical flow vectors for scene LC81940282015363LGN02 can be seen in Figure~\ref{fig:optical_flow_example}. In order not to visually flood the image with vectors for each pixel, they are dawn from 30 to 30 pixels (900 to 900 meters).
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=\columnwidth]{../images/LC81940282015363LGN02_NDSI_vectros_small.png}
		\caption{Overlayed motion vectors generated by optical flow for scene LC81940282015363LGN02.}
		\label{fig:optical_flow_example}
	\end{figure}

	\subsection{Generating predicted images based on motion}
	\label{seq:motion_ndsi_functional}
	
	\par Optical flow allows us to detect movement of the ice front, therefore we propose to generate NDSI images whose pixels will be predicted based on the past extracted motions for a frame. We can obtain the motion predicted NDSI image by relocating each pixel value from
	the NDSI(\(time=t+dt\)) to a location (\(x + 2*dx, y + 2*dy\)), thus generating the new image NDSI(\(time=t+dt\)) as described in Figure~\ref{fig:motion_generated_schema}, \(I(x + 2*dx, y + 2*dy, t + 2*dt)\). More methods of calculating the predicted location can be found in Section~\ref{seq:future_development}.
	
	\begin{figure}[h]
		\centering
		\includegraphics[scale = 0.5]{../images/motion_generated_schema.png}
		\caption{Overlayed motion vectors generated by optical flow for scene LC81940282015363LGN02.}
		\label{fig:motion_generated_schema}
	\end{figure}

	\par By applying this function to each pixel of the NDSI (\(t + dt\)) image and by making some adjustments which are further described in Section~\ref{seq:motion_ndsi_implementation}, for scene LC81940282015363LGN02, we have generated the motion predicted NDSI as shown in Figure~\ref{fig:motion_predicted_image}. The ice coverage for the generated image is 5.0099\% while the one for the actual NDSI(\(time=t+dt\)) is 5.3066\%.
	
	\begin{figure}[h!]
	\centering
	\includegraphics[width=\linewidth]{../images/LC81940282015363LGN02_motion_predicted.png}
	\caption{Motion generated NDSI for scene LC81940282015363LGN02.}
	\label{fig:motion_predicted_image}
	\end{figure}

	In order to better visualise the differences between the two images, they have been overlapped and colourized such that orange areas of snow that are predicted to melt, while blue represents areas which are considered to develop snow build up.
	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=\linewidth]{../images/LC81940282015363LGN02_motion_predicted_overlay.png}
		\caption{Overlapped motion generated NDSI for scene LC81940282015363LGN02.}
		\label{fig:overlapped_motion_predicted_image}
	\end{figure}

	\vfill
	% =========================================================================	

	\chapter{User Interface}
	\label{seq:gui}
	
	\section{Search and Download}
	\label{sec:search_download}
	
	\par Since the search and download part of the application can be viewed as a plug-in mechanism which only starts to query the Landsat archive , we have opted to keep its interface in the form of a command line one only. The user simply has to run the command showed Listing~\ref{lst:downloadcommand} in order to find out detailed descriptions of which are the available options for search and download, which will output the result in Listing~\ref{lst:downloadhelp}.
	
	\begin{lstlisting}[caption={Help command for sarch and download.},label={lst:downloadcommand},language=Bash]
		% bash -x run_download.sh --help
	\end{lstlisting}
	
	\begin{lstlisting}[caption={Detailed parameters required for the download script.},label={lst:downloadhelp},language=Bash]
		+ export PYTHONPATH=./sat-search:./sat-stac
		+ PYTHONPATH=./sat-search:./sat-stac
		+ python3 gits/__main__.py download --help
		usage: __main__.py download [-h] [--csv CSV] [-c C] [-d D] [-j J]
		
		optional arguments:
		-h, --help            show this help message and exit
		--csv CSV             Path to the csv file.
		-c C, --cloud-cover C
		Maximum cloud coverage allowed for a scene.
		-d D, --download-directory D
		Path to the output folder which will contain theprocessed results.
		-j J, --jobs J        Number of threads which will search and download.
	\end{lstlisting}

	When starting the scrip with the desired parameters, the process will query for assets and start downloading them, as shown in Figure~\ref{fig:download_cmd}.
	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=\columnwidth]{../images/download_cmd.png}
		\caption{Command line interface for the search and download part of the application.}
		\label{fig:download_cmd}
	\end{figure}

	\section{Processing}
	\label{sec:processing_gui}
	
	\par Our thesis focuses on computer vision, therefore we wanted to make sure that we are providing a tool which supports easy image visualisation, as well as an easy way to process. A graphical user interface which focuses on providing easy access to the glaciers dataset, equipped with a screen to easily visualise the images in different stages of processing proves to be the best approach for our purpose. Figure~\ref{fig:gui} represents the main use case scenario for using the interface, while also snowing snippets of NDSI images as well as generated ones and their snow coverage graphs.
	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=\columnwidth]{../images/GUI.png}
		\caption{Graphical user interface for the processing part of the application.}
		\label{fig:gui}
	\end{figure}

	\begin{lstlisting}[caption={Help command for processing.},label={lst:processcommand},language=Bash]
	% bash -x run_process.sh --help
	\end{lstlisting}

	In order to power up the application, we use the same approach as described in Section~\ref{subsec:search_download}, but running a different script with different parameters, described in Listing~\ref{lst:processcommand}. The only parameter necessary for starting the processing, as shown as an output to the command in Listing~\ref{lst:processhelp}, is the directory which holds the dataset which should have the same hierarchy as the one described in Figure~\ref{fig:DownloadDirectory}.

	\begin{lstlisting}[caption={Detailed parameters required for the download script.},label={lst:processhelp},language=Bash]
		+ export PYTHONPATH=./sat-search:./sat-stac
		+ PYTHONPATH=./sat-search:./sat-stac
		+ python3 gits/__main__.py process --help
		usage: __main__.py process [-h] [--input INPUT]
		
		optional arguments:
		-h, --help     show this help message and exit
		--input INPUT  Path to the input folder which contains the dataset of glaciers.
	\end{lstlisting}

	For more detailed information, the command line interface can still be used for various activity updates, processing time and possible errors, as shown in Figure~\ref{fig:process_cmd}.

	\begin{figure}[h!]
	\centering
	\includegraphics[width=\columnwidth]{../images/process_cmd.png}
	\caption{Command line interface for the processing part of the application.}
	\label{fig:process_cmd}
	\end{figure}

	% =========================================================================
	\newpage{}
	\chapter{Design and Implementation}
	\label{cha:design_and_implementation}
	
	\par The application design can be discussed by splitting the topics in:
	
	\begin{itemize}
		\item \textbf{search and download}
		\item \textbf{processing}
		\begin{itemize}
			\item Normalised Snow Difference Index;
			\item Alignment;
			\item Motion extraction through optical flow;
			\item Motion generated NDSI;
			\item Generated NDSI filtering.
		\end{itemize}
	\end{itemize}

	\par The first section is optional and can be run independently from the processing, since the user can have an already created database of images. However, we have focused on simplifying the process of satellite imagery downloading as described in Section~\ref{seq:STAC} through using the sat-search library as a helper for searching and downloading assets. More information on querying can be found in Section~\ref{seq:sd_implementation}. 
	Given an existing set of data, the next step is to pass the root folder which contains the glacier directories to the processing unit, also designed as a plug-in mechanism which will trigger the graphical user interface to pop up and allow for the processing options to appear. More information on this section can be found at Section~\ref{sec:processing_gui}. From there, one can start processing by simply making use of the predefined buttons described in Section~\ref{seq:gui}.
	
	% =========================================================================
		
	\section{Search and Download}
	\label{seq:sd_implementation}
		
	\par Searching as well as downloading have been implemented on top of the sat-search library and it is used directly from the command line interface by running the script described in Section~\ref{sec:search_download}.
	
	\par As an input we will be using a CSV file which will have the form as described in Section~\ref{seq:wgi}. Mainly we will need just four attributes to be specified for each desired glacier in order to create a query for searching, as follows: \textit{wgi\_glacier\_id, glacier\_name, lat and lon}.
	
	\par The CSV file will be intercepted by the \textbf{Download } class and sent for processing row by row (glacier by glacier) to the \textbf{GlacierFactory} class. This one is responsible for parsing each row of the input CSV file and transforming the information in \textbf{Glacier} objects, which will be passed back to the \textbf{Download} class.
	
	\par Using the newly created Glacier object we can construct its \textbf{search query} by calculating its bounding box, specifying the asset collection from which we request data and setting other parameters (maximum allowed cloud coverage, in our case). Listing~\ref{lst:query} represents an example of querying for glacier Belvedere, with the following parameters set in the CSV file: \textit{"IT4L01211009","BELVEDERE","45.942","7.908"}.
	
	\begin{lstlisting}[caption={Search query created by sat-search},label={lst:query},language=Bash]
		{"page": 1, "limit": 170, "bbox": [7.907990000000001, 45.94199, 7.90801, 45.94201], "query": {"eo:cloud_cover": {"lt": 10}}, "collection": "landsat-8-l1"}
	\end{lstlisting}

	\par The result of each glacier query will be automatically saved in a \textbf{JSON file} which is used as a data buffer between the search and download. The downloader takes each JSON file generated by the search engine and sends the command for getting that asset.
	
	\par The technical specifications of the Download, GlacierFactory and Glacier classes can be found in Figure~\ref{fig:sd_diagram}.
	
	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.6]{../images/sd_diagram.png}
		\caption{Technical specifications of Download, GlacierFactory and Glacier classes.}
		\label{fig:sd_diagram}
	\end{figure}

	\subsection{Data corruption verification}
	Both the searching and downloading functions are executed concurrently, since they are \textbf{time intensive} tasks. Each band has around 60 MB, which would make one scene approximately 360 MB. For the Belvedere glaciers, we found 78 scenes with a cloud coverage of 10\%. This means that the size of the entire glacier data will be around 28 GB. Given that one asset's size is quite large, it takes a lot of time to finish the download. In this time, even a small connection interruption might result in corrupted data files, which would interfere with processing. Therefore, we implemented an \textbf{extra security measure} in the sat-stac library which verifies whether a file with the same name already exists at the download location. If it does, its size will be compared to the size taken from the STAC-API servers. In case the two file sizes don't match, it means that the file got corrupted during download and it will be downloaded again. If the sizes match, the downloader will skip the file and continue when it finds one requested file which was not yet found on the disk, such that we do not end up unnecessarily overwriting the already existing files in case of failure.
	
	% =========================================================================

	\section{Processing}
	\label{seq:processing}

	\par Before generating any images, we first need to make sure that the pixels between any two scenes are not misaligned. The details of the alignment process are described in Section~\ref{seq:alignment_implementation}. After we ensure that out images are fit, we move on to creating the normalized snow difference index image in order to enhance the ice through a set threshold (see Section~\ref{seq:ndsi_implementation}). We then create the matrix of pixel motions calculated between two consecutive (date-wise) scenes, process described in Section~\ref{seq:motion_matrix_implementation}. Based on the information of where each pixel has moved between two consecutive images, we can then create our future NDSI image by applying the motion vectors on each pixel from a scene. More details on the design and implementation of this part can be found in Section~\ref{seq:motion_ndsi_implementation}.
	The normalized snow difference index image will be computed for each scene as described in Section~\ref{seq:ndsi_functional}.
	
	%========================================================================================
	
	\subsection{Alignment}
	\label{seq:alignment_implementation}\
	
	\par In order to keep a high level of abstraction, we have split each scene into aligned and unaligned: Scene and AlignedScene objects. These and their children are created when crawling through the glaciers directory, specifically for each region of interest. However, aligning all the images when crawling would result into a lot of idle time for the user when starting the application and it would not be overall feasible to do so. Therefore, a scene is aligned only when it is specifically being selected in the graphical user interface (or when another entity needs it, such as optical flow and image generation). By doing this, we ensure that there is no unnecessary extra waiting time for each scene that we have when powering up the interface. Figure~\ref{fig:alignment_diagram} contains the technical details of each class which takes part into the alignment process.
	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=\columnwidth]{../images/alignment_diagram.png}
		\caption{Technical diagram for the SceneInterface, AlignedScene, AlignedBand and Image classes.}
		\label{fig:alignment_diagram}
	\end{figure}
	
	\par As we have seen in Section~\ref{seq:landsat8_section}, the Landsat 8 satellite does not have a perfect trajectory, which results into misaligned images. We have solved this problem by using a strong \textbf{keypoint detection algorithm} which in our case detects edges formed by mountains and other geographical features present in the scenes. The \textbf{computer vision algorithms} used for this are \textbf{ORB (Oriented FAST and Rotated BRIEF)}, \textbf{Harris Corner Detector} and \textbf{RANSAC (Random Sample Consensus)} and they are applied on the raw 16bit grayscale image.
	
	\par \textbf{ORB} represents a fusion between the features extracted by using the fast accelerated
	segment test (FAST) keypoint detector combined with the binary robust independent
	elementary features (BRIEF) descriptor. The FAST detector finds keypoints in the
	image and uses Harris corner measure to select a number of top points from the
	generated list. We have found that keeping the top 25\% matches extracted from 5000 keypoints yielded the best results. Splitting
	the image into multiple boxes and applying the ORB algorithm on each separate
	part of the image such that features are homogeneously distributed also improved valid match making. As a final optimisation, we set the maximum allowed shifting euclidean distance between any two
	pixels to be at most 200, so 6km on the map, therefore getting rid of outlier matches
	based on the distance. The drawn matches between two images and a close up on the keypoints can be seen in Figure~\ref{fig:drawn_matches}.
	
	\begin{figure}
		\centering
		\begin{minipage}[0.2]{\columnwidth}
			\centering
			\includegraphics[width=\columnwidth]{../images/drawn_matches.png}
		\end{minipage}
		\begin{minipage}[0.2]{\columnwidth}
			\centering
			\includegraphics[width=\columnwidth]{../images/drawn_matches_small.png}
		\end{minipage}
		\caption{Drawn matches between scene LC81940282014216LGN01 and scene LC81940282013213LGN01 (top). Close up keypoints (bottom).}
		\label{fig:drawn_matches}
	\end{figure}
	
	\par The obtained matches can be then used to create an affine transformation matrix, generated through the \textbf{Random Sample Consensus (RANSAC)} algorithm, proposed by Fischler and Bolles in \cite{Fischler1981}. The affine transformation matrix describes the rotation and translation of the image to be aligned in comparison
	to its reference and it is used for warping it such that no two pixels are misaligned.
	%========================================================================================
	
	\subsection{Normalized Snow Difference Index}
	\label{seq:ndsi_implementation}
	
	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.7]{../images/ndsi_diagram.png}
		\caption{Technical diagram for the NDSI class.}
		\label{fig:ndsi_diagram}
	\end{figure}

	\par The NDSI is generated by using the Formula~\ref{eq:ndsi_formula} described in section \ref{seq:ndsi_functional} on the green and shortwave infrared bands from a specific scene. Each band pixel is converted to \textbf{float32} in order to increase the \textbf{precision} of calculation; as each band is read as a n-dimensional array we can use numpy's optimised processing for generating the NDSI index image, since it converts the data and runs on native code rather than going through Python's interpretor. Figure~\ref{fig:ndsi_diagram} holds the technical diagram for the NDSI.
	
	\par For easier visual analysis, we have coloured the result such that pixels which are considered to represent snow are white, while snow free land is represented with green. The resulting NDSI image can be viewed in Figure~\ref{fig:ndsi}. However, in he background, the raw NDSI image is used for better precision.

	%========================================================================================

	\subsection{Motion extraction through optical flow}
	\label{motion_ndsi_implementation}
	
	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.7]{../images/motion_vectors_diagram.png}
		\caption{Technical diagram for the MotionVectors class.}
		\label{fig:motion_vectors_diagram}
	\end{figure}

	\par As described in Section~\ref{seq:motion_ndsi_implementation}, in order to determine the motion vector for each pixel between two images, we have used \textbf{Gunnar Farneback's} algorithm for dense optical flow calculation, implemented by the \textbf{OpenCV} library. The MotionVectors class takes as input two NDSI images.
	
	\par Since the NDSI images have been aligned, depending on the affine transformation matrix, they will not overlap perfectly any more, resulting in \textbf{artifacts at their borders}. Applying optical flow by using two NDSI images as such would result in highly distorted motion vectors at the borders. For fixing this, we have created a \textbf{mask} from each image such that both of them are cropped with the mask of the other. This results in losing a small number of pixels at the borders which could not be taken into consideration anyway.
	
	\par The best results have been generated by using this method of generating the optical flow with a pyramid scale of 0.5
	and 6 pyramid levels, having in mind that the images that we work with are large. By
	choosing this combination of parameters, we state that at each level, the image is going
	to be reshaped at half the size of the previous one; therefore the area of search for motion
	is small enough such that the optical flow is able to track movement, with 3 iterations over each layer. The rest of the parameters have been been set at their typical values.
	
	\par As an addition to the vector visualisation, we have also implemented a different approach by using a colour coded
image. Colour intensity is directly proportional to the motion vector length, while its
hue represents the direction of this vector, as represented in Figure~\ref{fig:hue_colour_wheel}. The coloured image is highlighted in Figure~\ref{fig:colored_of}
	
	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.2]{../images/colorwheel.png}
		\caption{Hue representation of the optical flow.}
		\label{fig:hue_colour_wheel}
	\end{figure}
	
	\begin{figure}[h!]
		\includegraphics[width=\linewidth]{../images/LC81940282013341LGN01_Motion Vectros_color_small.png}
		\caption{Optical flow colourized motion vectors.}
		\label{fig:colored_of}
	\end{figure}
	
	%========================================================================================

	\subsection{Motion generated NDSI}
	\label{seq:motion_ndsi_implementation}
	
	\begin{algorithm}
			\SetKwInOut{Input}{Input}
			\SetKwInOut{Output}{Output}
			
			\underline{function generate} $(previous\_NDSI, motion\_vectors)$\;
			\Input{The NDSI(\(time=t+dt\) and the motion vectors generated by optical flow between NDSI(\(time=t\)) and NDSI(\(time=t+dt\))}
			\Output{The motion predicted NDSI(\(time=t+2*dt\)))}
			
			$motion\_predicted\_image \gets previous\_NDSI.shape$\;
			$width \gets NDSI.width()$\;
			$height \gets NDSI.height()$\;
			
			\For{$y$ in [$height, width$]}{
				\For{$x$ in [$height, width$]}{
					$dx \gets motion\_vectors[y][x][0]$\;
					$dy \gets motion\_vectors[y][x][1]$\;
	
					$new\_x \gets x + dx$\;
					$new\_y \gets y + dy$\;
					
					$motion\_predicted\_image[new\_y][new\_x] \gets previous\_NDSI[y][x]$\;
				} 
			}
			\Return{$motion\_predicted\_image$}\;

			\caption{Algorithm used for motion predicted image generation based on the optical flow vectors and NDSI(\(time=t+dt\))}
			\label{algo:change}
	\end{algorithm}
	
	\par As a first approach of populating the new image, we have simply iterated over all
	the pixels in the NDSI(\(time=t+dt\)) image and calculated their new coordinates
	based on their respective motion vector, as it can be seen in Algorithm~\ref{algo:iterative}. Since this
	is an iterative approach, it does not scale for images as large as the ones we are using.
	On top of this, since we are using the Python language, which is an interpreted one,
	each operation has to go through an interpreter before being processed. For very granular data
	processing this proves to be inefficient. For a typical image of 8543x8039 resolution, it
	took as much as 10 minutes for generating the image on the machine that we have used for processing (see Section\ref{seq:performance}).
	
	\par Since the data used for this thesis is large in size, relying on Python alone for data structures for processing does not scale. Python is an interpreted language, not a compiled one, which means that for each operation its interpreter has to do extra work such that it translated the bytecode instruction into a form that is machine executable. Since we have high resolution images, this does not scale. Instead, we resorted to using NumPy, which is an open-source library which brings support of large data computation, such as multi-dimensional arrays and matrices, as well as a large collection of mathematical functions which can be easily used for their operation \cite{HARRIS2020}. The library has a \textbf{well optimised C code core} which can handle large processing by bypassing the python interpreter, which results in the speed of a compiled language, not an interpreted one. Therefore, based on the needs of our dataset, we have used NumPys' main data structure, the ndarray (n-dimensional array) as a data structure to hold our images and various optimisations applied throughout processing focused on using only this type of data structure in order to make use of NumPys' optimisations and speed. By doing this instead of simply iterating, we were able to improve the processing time by around 1700\%, bringing it down approximately from 10 minutes to 35 seconds. 
	
	\par In the iterative approach, for each pixel we add its motion to its position such that we get its new location. These numbers can be computed ahead of time and transformed into arrays such that we only use optimised operations. By creating an \textbf{array of the initial coordinates} (\(x, y\)) and \textbf{adding the motion vectors} (\(dx, dy\)) array to it, we \textbf{generated the new coordinates} where each pixel from the NDSI image should be translated. By adding this modification we got rid of lines 5, 6, 7, 8, 9, 10 and 11 from the algorithm and replaced them with the code as it can be seen in Algorithm~\ref{algo:improved_generation}. The \(new\_coordinates\) and \(NDSI\) can be treated as a sparse array which is then densified using numpy capabilities. The technical description of the MotionPredictedNDSI class can be found in Figure~\ref{fig:motion_predicted_diagram}.
	
	
	\begin{algorithm}
		\SetKwInOut{Input}{Input}
		\SetKwInOut{Output}{Output}
		
		\underline{function generate} $(NDSI, motion\_vectors)$\;
		\Input{The NDSI(\(time=t+dt\) and the motion vectors generated by optical flow between NDSI(\(time=t\)) and NDSI(\(time=t+dt\))}
		\Output{The motion predicted NDSI(\(time=t+2*dt\)))}
		
		$motion\_predicted\_image \gets NDSI.shape$\;
		$width \gets NDSI.width()$\;
		$height \gets NDSI.height()$\;
		
		$index\_array \gets [[[0, 0], [1, 0] ... [width, 0]$
		$[0, 1], [1, 1] ... [width, 1]$
		$...$
		$[0, height], [1, height] ... [width, height]]]$\;
		$new\_coordinates \gets motion\_vectors + index\_array$\;
		
		$motion\_predicted\_image[new\_coordinates] \gets NDSI$\;
		
		\Return{$motion\_predicted\_image$}\;
		
		\caption{Improved algorithm used for motion predicted image generation based on the optical flow vectors and NDSI(\(time=t+dt\))}
		\label{algo:improved_generation}
	\end{algorithm}
	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=\linewidth]{../images/LC81940282015363LGN02_Motion Predicted NDSI_unfiltered_small.png}
		\caption{Raw generated NDSI image, unfiltered, zoomed in for a clear view.}
		\label{fig:unfiltered_small}
	\end{figure}
	
	
	\par The function used to generate the predicted NDSI does not have a \textbf{one-to-one domain}, thus making it undefined for certain inputs. This results in a very \textbf{noisy} generated image, as it can be seen in Figure~\ref{fig:unfiltered_small}. We have implemented a \textbf{filter} which uses the weighted average of the neighbouring pixels values to fill the missing ones.
	
	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.7]{../images/motion_created_diagram.png}
		\caption{Technical diagram for the classes involved in generating the motion extracted images.}
		\label{fig:motion_predicted_diagram}
	\end{figure}
	
	\subsection{Generated image filtering}
	\label{seq:filter}
	
	\par A first step into creating the filter for the motion predicted NDSI image is to create a \textbf{mask} which will be applied on the black border of the scene such that we are looking for \textbf{undefined values} only inside it. Using numpy, the \textbf{coordinates} of these pixels are extracted into an array which then can be processed in parallel by Pythons' \textbf{multiprocessing} library. We have used multiprocessing instead of threading because Python does not have true parallelization in multithreading, only concurrency, which would not be a real improvement on the processing time. Since all the processes have to write different chunks of the same image and they do not share the same memory space, we chose to solve this problem by creating a shared memory buffer to hold the image.
	
	\par The value of each found undefined pixel has to be calculated as an \textbf{weighted average} composed of its \textbf{neighbouring pixels}, as shown in Figure~\ref{fig:gaussian} top. We created the \textbf{kernel} which holds the weight of each pixel in the neighbourhood such that pixels closer to the centre have a higher weights than those near the edge. However, we have the case when we have multiple undefined pixels in the same neighbourhood. Since we cannot take their values in consideration, we do not want to include them in the weighted average, thus we set their weight to be 0, as shown in Figure~\ref{fig:gaussian} left, bottom. The result of the weighted average will be then stored as the value of the currently focused undefined pixel, as highlighted in Figure~\ref{fig:gaussian} right, bottom.
	
	\begin{figure}[h!]
		\centering
		\begin{minipage}{0.45\columnwidth}
			\centering
			\includegraphics[width=\linewidth]{../images/gaussian_image.png}
		\end{minipage}
		\begin{minipage}{0.45\columnwidth}
			\centering
			\includegraphics[width=\linewidth]{../images/gaussian_kernel1.png}
		\end{minipage}
		\begin{minipage}{0.45\columnwidth}
			\centering
			\includegraphics[width=\linewidth]{../images/gaussian_weights.png}
		\end{minipage}
		\begin{minipage}{0.45\columnwidth}
			\centering
			\includegraphics[width=\linewidth]{../images/gaussian_pixel.png}
		\end{minipage}
		\caption{Zoomed in part of the predicted NDSI (left, top). Neighbourhood extracted (right, top). Kernel of weights (left, bottom). Generated value (right, bottom).}
		\label{fig:gaussian}
	\end{figure}

	% ===================================================================================
		
	\section{Tools and libraries}
	\label{seq:extras}
	
	In the following sections we will address the tools and libraries which were used as a support for building the application.
	
	\subsection{Programming environment}
	\label{seq:programmingenv}
	
	\par The application part of the thesis has been written with the help of the \textbf{Emacs} text editor, mainly because it provides a uniformity which supports various working flows, since it is not focused on a specific programming language, like PyCharm or Eclipse, for example. The editor is highly customizable and it provides structured, powerful commands for text editing, file searching, in-place code versioning and many more.
	
	\subsection{Programming language}
	\label{seq:programminglang}
	
	\par For the purpose of the thesis we have used the \textbf{Python} programming language mainly because of its numerous available libraries which support computer vision, heavy mathematical computations and raster image processing. The most important libraries used in this application are listed at Section~\ref{seq:libraries}.
	
	\subsection{Libraries}
	\label{seq:libraries}
	
	\subsubsection{NumPy}
	
	\par Since the data used for this thesis is large in size, relying on Python alone for data structures for processing does not scale. Python is an interpreted language, not a compiled one, which means that for each operation its interpreter has to do extra work such that it translated the bytecode instruction into a form that is machine executable. For a large number of operation, such as we have high resolution images, this does not scale. Instead, we resorted to using NumPy, which is an open-source library which brings support of large data computation, such as multi-dimensional arrays and matrices, as well as a large collection of mathematical functions which can be easily used for their operation \cite{HARRIS2020}. The library has a \textbf{well optimised C code core} which can handle large processing by bypassing the python interpreter, which results in the speed of a compiled language, not an interpreted one. Therefore, based on the needs of our dataset, we have used NumPys' main data structure, the ndarray (n-dimensional array) as a data structure to hold our images and various optimisations applied throughout processing focused on using only this type of data structure in order to make use of NumPys' optimisations and speed.
	
	\subsubsection{Open Source Computer Vision Library (OpenCV)}
	
	\par The OpenCV library brings support for \textbf{computer vision processing}, by providing programming functions which can be reliably used \cite{cuda}. Its use in our thesis both for image alignment and detecting motion through deep optical flow, by using its already integrated functions as a support. OpenCV, as well as NumPy, is open-source and it is written and optimised in native code, which is needed for the large size of our dataset entities.
	
	\subsubsection{Geospatial Data Abstraction Library (GDAL)}
	
	\par The GDAL library is a translator for vector and \textbf{raster geo-spatial data formats}, which presents a single data model to work with \cite{gdal}. Since our images are represented in the tiff format, reading them by using GDAL proved to be the easy and reliable.
	
	\subsection{Version-control}
	\label{seq:versioning}
	
	\par In order to keep track of various changes, bugs and enhancements of our application, we had to make sure that we are using a \textbf{reliable version-control} system. For this purpose, Git and the repository containing the application can be found on Github, at \textbf{\url{https://github.com/BabyCakes13/GlacierImagePredictor}}.
	
	\newpage{}
	\chapter{Performance and experiments}

	
	\section{Performance}
	\label{seq:performance}
	
	\par In order to lower the user time while inspecting the data we are caching on storage the results of processing in multiple stages. The saved images are all the aligned ones, as well as the NDSI and generated NDSI. Therefore, before inspecting the data, the user has the option of issuing the cache command, which will go through all the dataset and compute the results. The caching is also done without issuing the manual cache command on the whole dataset, while simply using the graphical user interface. This helps to create an undisrupted user experience even when handling computationally heavy data.
	
	For the performance evaluation we are using a machine which has an Intel i7-7700HQ CPU, with 24 GB of RAM. The images were stored on my personal server and they were locally mounted through NFS.
	
	Downloading the dataset of two glaciers (IN5Q31300046, IT4L01211009) which have been used for the experiment section (Section~\ref{seq:experiments}) took around two hours on my machine, while their searching took around half an hour. For IN5Q31300046 (Parvati), there were 185 entries, while for IT4L01211009 (Jungfrau-Aletsch-Bietschhorn), there were 109. The processing and caching of all the datasets took around 8 hours, which outputs around 1 minute and 30 seconds per scene.
	
	\section{Experiments}
	\label{seq:experiments}
	
	\par We have picked two glaciers and downloaded all their public available satellite imagery with at most 20\% cloud coverage. We have split our dataset based on \(path, row, month\) pairs, as suggested in \cite{RACOVITEANU2019}, due to different seasonal snow coverage fluctuations. In order to better visualise the differences between the two images, they have been
	overlapped and colourized such that orange areas of snow that are predicted to melt,
	while cyan represents areas which are predicted to develop snow build up.
	
	\par The first glacier region which will be analysed is named \textbf{Jungfrau-Aletsch-Bietschhorn}, located in the \textbf{Swiss Alps}, specifically at latitude 46.47735081308319 and longitude 8.056887228860798, with an elevation which varies between 809 and 4,274 m. This location is at the intersection of multiple WRS-2 coordinates, therefore we will have different sets of \(path, row, month\) which we will be focusing on.
	
	\subsection{Jungfrau-Aletsch-Bietschhorn (194, 28, 4)}
	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=\linewidth]{../images/experiment_1940284_image.png}
		\caption{Overlapped motion generated NDSI for scene LC81940282017112LGN00.}
		\label{fig:experiment_1940284_image}
	\end{figure}

	The motion generated image for this path and row can be seen in Figure~\ref{fig:experiment_1940284_image}, while and Figure~\ref{fig:jungfrau_194284} holds the snow coverage values extracted from each image in the dataset. This scene was captured in April, when the temperatures are still low and the snow fall fluctuates a lot, resulting in a low signal to noise ratio. This pattern proved to yield inconsistent results for the generated images.
	
	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.5]{../images/experiment_194284.png}
		\caption{Comparison between the NDSI values of the actual NDSI images for Jungfrau-Aletsch-Bietschhorn(194, 28, 4), and the NDSI of each predicted motion image.}
		\label{fig:jungfrau_194284}
	\end{figure}

	% =================================================================
	
	\newpage{}
	
	\subsection{Jungfrau-Aletsch-Bietschhorn (194, 28, 7)}
	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=\linewidth]{../images/experiment_1940287_image.png}
		\caption{Overlapped motion generated NDSI for scene LC81940282020201.}
		\label{fig:experiment_1940287_image}
	\end{figure}
	
	Figure~\ref{fig:experiment_1940287_image} represents the motion predicted NDSI image afferent to this path, row and month pair. As it can be seen in Figure~\ref{fig:jungfrau_194287}, the scene was captured in July, hence the more consistent snow fall. These results yield a better estimation of movement than the one obtained in early spring.
	
	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.5]{../images/experiment_194287.png}
		\caption{Comparison between the NDSI values of the actual NDSI images for Jungfrau-Aletsch-Bietschhorn(194, 28, 7), and the NDSI of each predicted motion image.}
		\label{fig:jungfrau_194287}
	\end{figure}

	% ===================================================================
	
	\newpage{}
	
	\subsection{Jungfrau-Aletsch-Bietschhorn (194, 28, 8)}
	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=\linewidth]{../images/experiment_1940288_image.png}
		\caption{Overlapped motion generated NDSI for scene LC81940282016238LGN01.}
		\label{fig:experiment_1940288_image}
	\end{figure}

	The result of the experiment is captured in Figure~\ref{fig:experiment_1940288_image}, while its graph of snow coverage can be located at Figure~\ref{fig:jungfrau_194288}. The image was captured during August, which in this case has the least amount of snow fall. The result for this data set was the most reliable, as one can observe since the moraines are clearly exposed. 
	
	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.5]{../images/experiment_194288.png}
		\caption{Comparison between the NDSI values of the actual NDSI images for Jungfrau-Aletsch-Bietschhorn(194, 28, 7), and the NDSI of each predicted motion image.}
		\label{fig:jungfrau_194288}
	\end{figure}
	
	% ==================================================================
	
	\newpage{}
	
	\subsection{Jungfrau-Aletsch-Bietschhorn (195, 28, 9)}
	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=\linewidth]{../images/experiment_1950289.png}
		\caption{Overlapped motion generated NDSI for scene LC81950282020256.}
		\label{fig:experiment_1950289_image}
	\end{figure}

	The last presented data set was acquired in September when the probability of snow fall increases. This is highlighted by the entry in 2017 which represents an outlier.
	
	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.5]{../images/experiment_195289.png}
		\caption{Comparison between the NDSI values of the actual NDSI images for Jungfrau-Aletsch-Bietschhorn(195, 28, 9), and the NDSI of each predicted motion image.}
		\label{fig:jungfrau_195289}
	\end{figure}
	
	\vfill
	\newpage{}
	
	\par The second glacier which we have taken into consideration is named \textbf{Parvati} and it is located in \textbf{India}, at latitude 31.754 and longitude 77.675, with a maximum elevation of 5599 meters. We have chosen this glacier such that we can compare our results with the ones provided in \cite{parvati}. They have chosen a dataset of images located at WRS-2 path 147, row 38. However, we could not fetch the same images for that specific path and row in order to be able to make this comparison. The result that were achieved are however listed below for the available path and row.
	
	\subsection{PARVATI (146, 38, 4)}
		
	\begin{figure}[h!]
		\centering
		\includegraphics[width=\linewidth]{../images/experiment_1460384_image.png}
		\caption{Overlapped motion generated NDSI for scene LC81460382021091.}
		\label{fig:experiment_1460384_image}
	\end{figure}

	
	Here we can see again that the high snow fluctuation yields in big differences between the predicted image and the actual one, as it can be observed in Figure~\ref{fig:experiment_1460384_image}. Given that the Parvati region is located at a higher altitude than the Jungfrau-Aletsch-Bietschhorn one, its snowfall is higher. This can also be seed while analysing the snow coverage values for the values from Figure~\ref{fig:jungfrau_194284} and Figure~\ref{fig:parvati_146384}, where the maximum snow fall for Parvati is around 57\% (2014) and the one for Jungfrau-Aletsch-Bietschhorn is around 23\% (2016).

	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.5]{../images/experiment_146385.png}
		\caption{Comparison between the NDSI values of the actual NDSI images for Parvati(146, 38, 4), and the NDSI of each predicted motion image.}
		\label{fig:parvati_146384}
	\end{figure}
	
	% ==============================================================

	\newpage{}	
	\chapter{Conclusions}
	\label{seq:conclusions}
	
	\par The best and consistent results were obtained for scenes which were acquired between April and September, due to lower snow fall and better glacier exposure. We have found that scenes which are taken during the winter months are prone to erroneous results due to higher snow coverage which creates high fluctuations and outliers. In \cite{TAK2020}, only September was taken into consideration when conducting the experiments, mainly because it is the month which has the lowest recorded snow coverage of the area. Also, in In \cite{RACOVITEANU2019}, the experiments were performed on seasonal organized data and the scenes with higher snow coverage proved problematic as well. Another factor which influenced the results is cloud coverage, since even if the shortwave infrared band filters out most of the clouds found in a scene, still there are some cases when the coverage is very dense and the land underneath cannot be analysed. Since the terrain is not visible, the glacier pixels cannot be properly extracted, resulting in erroneous snow coverage and distorted motion generated images at the locations where the clouds have appeared or disappeared.
	
	\section{Future development}
	
	\par During the process of creating the application, multiple problems were met. One of them was that even though the number of available aerial images collected in the Landsat 8 archive are high, most of them have very high cloud coverages. Filtering out results which have lower percentages create datasets with low number of entities, which means that we cannot test our results on longer time series. Since clouds interfere both with extracting glacier pixels from a scene and analysing motion, two approaches on solving this problem would be:
	
	\begin{itemize}
		\item harvesting images provided by \textbf{multiple satellites}, such as Sentinel and older versions of Landsat, as proposed in \cite{WINSVOLD2017}. Even if these satellites use other types of sensors for Earth observation, by matching the wavelengths of their bands one could organize and pair their datasets, resulting in time series with more entities and more scenes which have lower cloud coverage. By only using data provided by the Landsat 8 satellite, even with an allowed cloud coverage of 20\% as we have used for our experiments, the average length of a \(path, row, month\)) dataset was around 15;
		\item implementing \textbf{cloud mapping algorithms} such that clouds could be detected and trimmed out before the calculation of the normalized snow difference index and optical flow extraction. For now, the optical flow algorithm ran between scenes which have a high cloud coverage finds large vectors of motion at the locations where clouds existed from one frame to another. The result does not take into consideration the difference between snow pixels and cloud ones. 
	\end{itemize}
	
	\par Another improvement could be changing the way that the coordinates of the pixels of the motion generated image are calculated. An interesting approach would be using \textbf{time series forecasting models} applied on the distance vectors generated by optical flow for each pixel over time in order to generate future entries. This could be done with statistical methods such as the autoregressive integrated moving average (ARIMA). However, doing this forecast over each pixel of a high resolution image would take a lot of processing power; also, a larger \(path, row, month\) dataset would be needed for such an analysis.
	
	\par One of the main slow downs in developing and using the application was the slow time of processing due to the large data files. A machine which has a more powerful processing unit would yield much faster results. This could be done by \textbf{migrating} the processing unit to \textbf{cloud} and hiring a machine with a CPU which has a high number of cores and better performance. One of the possible downsides would then be the amount of time needed for passing the large scene files through he network to the machine on cloud, which is easily dependable on the bandwidth. A solution to this downside would be passing the dataset before starting the processing and make sure that they are on the same machine. However, cloud storage can get very expensive and the trade off between processing time and expenses could prove too unbalanced.
	
	\par As for a final future development idea, migrating the codebase from Python to a language which supports multithreading as well as compiling to machine language, such as C++, would yield in much faster results. 

	
	\newpage{}
	\chapter{Glossary}
	
	% ==================================================================
	
	\section{Acronyms}
	
		\begin{table} [H]
		\centering
		\begin{tabular} {|  l | L{10cm} |}
			\hline
			WGI & World Glacier Inventory \\ [0.2ex]
			\hline
			NDSI & Normalized Snow Difference Index \\ [0.2ex]
			\hline
			CSV & Comma separated values \\ [0.2ex]
			\hline
			JSON & JavaScript Object Notation \\ [0.2ex]
			\hline
			NFS & Network File System \\ [0.2ex]
			\hline
			NASA & National Aeronautics and Space Administration \\ [0.2ex]
			\hline
			WRS-2 & Worldwide Reference System-2 \\ [0.2ex]
			\hline
		\end{tabular}
		\caption{Acronyms table }
		\label{table:acron}
	\end{table}

	\bibliographystyle{apalike}
	\bibliography{references}

	\end{document}

