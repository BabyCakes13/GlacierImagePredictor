\documentclass[11pt, a4paper]{report}
\special{papersize=210mm, 297mm}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[left=2.5cm, right=2.5cm, top=2.5cm]{geometry}
\renewcommand{\baselinestretch}{1.4}
\usepackage[toc,page]{appendix}
\pagenumbering{alph}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\graphicspath{{images/} {diagrams/}}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage[linesnumbered,ruled]{algorithm2e}

%\usepackage{showframe}
\usepackage{fullpage}

\usepackage{url}
%\usepackage{natbib} % for author-date citation \citep{}, \citet[]
%\usepackage{hyperref}
%\usepackage[nottoc]{tocbibind}
\usepackage{listings}
\usepackage{verbatim}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
	language=java,
	aboveskip=5mm, belowskip=3mm, showstringspaces=false,
	columns=flexible, basicstyle={\small\ttfamily},
	numbers=none, numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=3
}


\usepackage{multirow}
\usepackage{array}
\newcolumntype{L}[1]{> {\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\pagenumbering{arabic}

\begin{document}
	
	%======================================================================
	
	\begin{titlepage}
		
		\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
		
		\center
		
		\vspace{-20pt}
		\includegraphics[width=100pt]{../images/FMI-03.png}\\[1.0cm]
		
		\textsc{\LARGE West University of Timisoara}\\[0.5cm]
		\textsc{\Large Faculty of Mathematics and Computer Science}\\[0.5cm]
		\textsc{\large Study Program: \\Computer Science in English}\\[3cm]
		\textsc{\Huge Master Dissertation}\\[5cm]
		
		\begin{minipage}{0.4\textwidth}
			\begin{flushleft} \large
				\textbf{COORDINATOR:}\\
				Associate Prof. Marc Eduard \textsc{Frîncu}
			\end{flushleft}
		\end{minipage}
		~
		\begin{minipage}{0.4\textwidth}
			\begin{flushright} \large
				\textbf{GRADUATE:} \\
				Maria Minerva \textsc{Vonica}
			\end{flushright}
		\end{minipage}\\[0.5cm]
			
		\vfill
		{\large Timi\c{s}oara \\2021}\\
		\vfill
		
	\end{titlepage}
	
	% =====================================================================
	
	\begin{titlepage}
		
		\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
		
		\center
		
		\textsc{}\\[.7cm]
		
		\textsc{\LARGE West University of  Timi\c{s}oara}\\[0.5cm]
		\textsc{\Large Faculty of Mathematics and Computer Science}\\[0.5cm]
		\textsc{\large Study Program: \\Computer Science in English}\\[4.5cm]
		
		\textsc{\Huge Master Dissertation}\\[2cm]
		
		{\Huge \bfseries Name}\\[6cm]
		
		\begin{minipage}{0.4\textwidth}
			\begin{flushleft} \large
				\textbf{COORDINATOR:}\\
				Associate Prof. Marc Eduard \textsc{Frîncu}
			\end{flushleft}
		\end{minipage}
		~
		\begin{minipage}{0.4\textwidth}
			\begin{flushright} \large
				\textbf{GRADUATE:} \\
				Maria-Minerva \textsc{Vonica}
			\end{flushright}
		\end{minipage}\\[0.5cm]
		\vfill
		{\large Timi\c{s}oara\\ 2021}\\
		
		\vfill
		
	\end{titlepage}

	% =====================================================================
	
	\begin{abstract} %begin abstract  
		%The abstract should have one page and should be a compact presentation of the dissertation.
		\vspace{1.0cm}
		\vspace{1.0cm}
		\par
		
			
		Over the last decade, climate change has impacted Earths' atmosphere and environment more than anytime before. Studies held by NASA confirmed that the second hottest year ever recorded was 2019 and the recorded temperatures of the ocean have never been higher. One way to track these changes is to study the retreat of glaciers, since they are the most sensitive climate change indicators. Since NASA and USGS launched their first Earth observation satellite in 23 July 1972, millions of images of the Earth have been stored and are freely to use for various fields of research. 
		\vskip 0.2cm
		Having this in mind, the goal of our application is to analyse the retreat of glaciers over time by using advanced computer vision algorithms which are able to detect movement of pixels based on a time series of aerial scenes collected from the Landsat 8 satellite. By analysing the directions of glacier retreat over time, we try to generate future images of how they might change. The snowfall and ice percentages of a glacier for a satellite image are also taken into consideration as a validation of our results. The application has also been made with the goal of being simple to use and easy to access for anyone who wants to get involved and make a contribution. 
		\vskip 0.2cm
		This theses contains information about the application's functionality, processing mechanics and experiments on various glaciers. In order to have a self sufficient environment,  both search and download of Landsat 8 assets facilities have been made available, such that one does not need to have an already created dataset of glacier satellite images.  As for generating a motion predicted image, we need to extract movement information between two consecutive scenes which have had their snow and ice pixels highlighted; the scenes are then passed to the computer vision algorithm which extracts the motion (optical flow) of each pixel and vectors of movement are generated. The results are used to further extend this movement trend which generates a predicted image.
		
	\end{abstract} %end abstract

	\section{The context of this thesis} 

	Climate change has become one of the biggest problems in the past years, human activities being one of the main factors by inducing global warming \cite{climate_change_1}. Due to heavy industrialisation of the society since the Industrial Revolution around the VII century, the levels of carbon dioxide in the atmosphere have grown beyond the capacity of natural elimination, creating the greenhouse effect. 
	
	The greenhouse effect is the process in which the radiation from a planet's atmosphere warms the planet surface above that normal temperature \cite{greenhouse}. The main causes of this change are human activities such as combustion of fossil fuels (coal, oil, natural gas), deforestation, soil erosion and agriculture \cite{climate_change_2}. If the change continues at this rate, there is an estimation that the average surface temperature will surpass \textdegree{3}  C this century \cite{climate_change_3}. Though it may not seem a lot, there is a scenario created over 30 years into the future explaining what this increase actually means: from 2020 to 2030 the temperature is set to rise above \textdegree{1.6}C; by this report, it is estimated that the average temperature will increase with \textdegree{3}C around 2050, in which case even if gas emissions are stopped, there will be another rise of at least \textdegree{1}C \cite{climate_change_4}. 
	
	One of the ways of creating an overview of the last decades of climate change is by analysing the change in the glaciers, since they are among the most sensitive indicators of climate change \cite{glacier_1}. The retreat of a glacier is caused by an increase of the temperature and less snowfall, a process which has been accelerated recently, due to the global warming \cite{glacier_2}.
	
	In the context of these changes, which are already beginning to impact the society and environment, I strongly believe that we have to take advantage of every opportunity to better understand and predict these changes, so proper preparations and actions could be taken as soon as possible. 


	\section{Our Contribution}
	
	This paper uses image analysis techniques paired with artificial intelligent algorithms to create a time series analysis on the snow coverage of glaciers around the globe, while using methods which prove to be highly performant and accurate:
	\begin{itemize}
		\item Alignment of images with accuracy rate of \textbf{98\%} percent for the successful processed ones;
		\item Successful alignment of approximately \textbf{85\%} images for a Landsat 8 iamge;
		\item Snow coverage prediction with approximate \textbf{20\%} mean error.
	\end{itemize}.
	The results were achieved by calculating the NDSI for extracting snow pixels from a Landsat 8 image on a set of images for a glacier which has been split into path and row coordinates, paired up with alignment of the resulting images for displaying their differences over the years. The Normalized Difference Snow Index (NDSI) was obtained by calculating a formula which uses the green and SWIR1 bands from a scene and applying a threshold in order detect snow. The method of snow extraction was chosen to be NDSI calculation due to snow's high reflectance in the visible spectrum and high absorption in the short-wave infrared one. 
	\vskip 0.2cm
	After the snow coverage has been extracted, the NDSI, green and SWIR1 images are aligned based on a reference image, such that creation of an image which highlights the presence or absence of snow between two scenes, and one with highlight the movement can be created without pixel discrepancy errors. By reducing the misalignment to almost 0 shift, the results are reliable. 
	Prediction of snow coverage is done by creating a data set containing snow ratio information extracted from the NDSI images, which is normalized (outliers are removed) and split into a train section and a test one, also having future predictions. The results are plotted for easier data interpretation and they highlight the pattern of snow coverage in the past years, as well as future ones, with a small error percent (approximately 20\%). An exact estimation of the error rate cannot be given due to the fact prediction is highly dependent on NDSI values, which are based on a high number of glacier variables such as the quality of image, the seasonal variation and other factors.  
	\vskip  0.2cm
	The thesis application can be used as an command line interface, as well as a graphical user one, in order to include a user group of both specialized and non-specialized in the field. 
	.
	\newpage{}

	
	\tableofcontents{}
	\addcontentsline{toc}{chapter}{List Of Figures}
	\listoffigures{}
	\addcontentsline{toc}{chapter}{List Of Tables}
	\listoftables{}
	
	\newpage{}
	
	\chapter{Application}
	\section{Dataset}
	In order to build the dataset, we made use of the freely available data from the Landsat Archive, specifically from collection 1, level 1. This consists of data products generated from Landsat 8 Operational Land Imager/Thermal Infrared Sensor, Landsat 7 Enhanced Thematic Mapper Plus, Landsat 4-5 Thematic Mapper, and Landsat 1-5 Multispectral Scanner instruments \cite{C1L1}. For the purpose of this paper, we will focus only on images collected from the Landsat 8 satellite.
	
	\subsection{Landsat 8}
	\label{seq:landsat8_section}

	For the purpose of this paper, we will use images collected by the Landsat 8 satellite Figure~\ref{fig:landsat_satellite}, which was launched on an Atlas-V rocket from Vandenberg Air Force Base, California on February 11, 2013. Landsat 8 is the most recently launched Landsat satellite and carries the Operational Land Imager (OLI) and the Thermal Infrared Sensor (TIRS) instruments.
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.25]{../images/LandsatSatellite.png}
		\caption{Landsat 8 satellite \cite{LANDSATPIC}}
		\label{fig:landsat_satellite}
	\end{figure}
	Landsat 8 orbits the Earth in a sun-synchronous, near-polar orbit, at an altitude of 705 km, inclined at 98.2 degrees, and completes one Earth orbit every 99 minutes.  The satellite has a 16-day repeat cycle with an equatorial crossing time: 10:00 a.m. +/- 15 minutes. It acquires about 740 scenes a day on the Worldwide Reference System-2 (WRS-2) path/row system, with a swath overlap (or sidelap) varying from 7\% at the equator to a maximum of approximately 85\% at extreme latitudes. A Landsat 8 scene size is 185 km x 180 km \cite{LANDSAT}.
	
	\paragraph{Worldwide Reference System-2}
	\label{par:wrs-2}
	
	We will be referring to each scene's location based on its path and row coordinates from the worldwide reference system-2, which is a global notation system for Landsat data. It enables a user to inquire about satellite imagery over any portion of the world by specifying a nominal scene center designated by path and row numbers. The WRS has proven valuable for the cataloguing, referencing, and day-to-day use of imagery transmitted from the Landsat sensors \cite{wrs}. Landsat's trajectory projected on the world map can be seen in Figure~\ref{fig:wrs2}.
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.2]{../images/wrs2.png}
		\caption{WRS-2 Path/Row for Landsat \cite{wrs}.}
		\label{fig:wrs2}
	\end{figure}
	
	\subsubsection{Landsat scene naming convention}
	\label{seq:landsat_naming}
	
	Each Landsat scene is named after a well-defined convention in order to easily check information such as the WRS path, row and the date of acquisition. Having access to this information without the need to download the scene itself or the metadata file which holds this information represents a valuable asset, since we can easily filter the data based on the naming convention itself. In the table \ref{table:scene_naming} below we represent what each part of a Landsat scene of the form \textbf{LXS PPPRRR YYYYDDD  GSIVV} means.
	\begin{table} [h]
		\center
		\begin{tabularx}{480pt}{|X|X|}
			\toprule
			\textbf{L} & Landsat \\ [0.2ex]
			\midrule
			\textbf{X} & Sensor \\ [0.2ex]
			\midrule
			\textbf{SS} & Satellite \\ [0.2ex]
			\midrule
			\textbf{PPP} & WRS path \\ [0.2ex]
			\midrule
			\textbf{RRR} & WRS row \\ [0.2ex]
			\midrule
			\textbf{YYYY} & Acquisition year \\ [0.2ex]
			\midrule
			\textbf{DDD} & Julian day of the acquisition year \\ [0.2ex]
			\midrule
			\textbf{GSI} & Ground station identifier \\ [0.2ex]
			\midrule
			\textbf{VV} & Archive version number \\ [0.2ex]
			\midrule
			\midrule
			\bottomrule
		\end{tabularx}
		\caption{Landsat 8 scene naming convention \cite{sn}.}
		\label{table:bands_table}
	\end{table}\label{table:scene_naming}
	
	\subsubsection{Operational Land Imager}
	
	The Operational Land Imager (OLI) is a remote sensing instrument aboard Landsat 8, built by Ball Aerospace \& Technologies. The sensor collects moderate resolution data that is used to monitor changing trends on the surface and evaluate how land usage changes over time. The images and data that OLI has helped collect have practical applications today in agriculture, mapping, and monitoring changes in snow, ice, and water \cite{lolidcp}. 
	The OLI operates in the visible (VIS) and short wave infrared (SWIR) spectral region, having a width of 185 km. It uses nine channels, which range from wavelengths of 443 nm to 2,200 nm. Of these nine channels, eight are multispectral and one is panchromatic. The eight multispectral channels have a 30-meter spatial resolution, and the panchromatic channel has a 15 meters one.
	The OLI generates 9 bands for Landsat as shown in Figure~\ref{fig:L8OLI}.
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.5]{../images/Landsat8-OLI-Bands.png}
		\caption{Landsat 8 OLI generated bands \cite{l8otb}}
		\label{fig:L8OLI}
	\end{figure}
	
	\subsubsection{Thermal Infrared Sensor}
	
	The Thermal Infrared Sensor (TIRS) measures land surface temperature in two thermal bands with a new technology that uses Quantum Well Infrared Photodetectors to detect long wavelengths of light emitted by the Earth whose intensity depends on surface temperature. These wavelengths, called thermal infrared, are well beyond the range of human vision \cite{lgng}.
	The thermal infrared sensor generates 2 bands for Landsat as shown in Figure~\ref{fig:L8TIRS}.
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.5]{../images/Landsat8-TIRS-Bands.png}
		\caption{Landsat 8 TIRS generated bands \cite{l8otb}}
		\label{fig:L8TIRS}
	\end{figure}
	
	\subsubsection{OLI and TIRS bands}
	
	Landsat 8 acquires data from these sensors in in 11 bands, as following in Table~\ref{table:bands_table}.
	
	\begin{table} [h]
		\center
		\begin{tabularx}{480pt}{|p{0.3\linewidth}|X|X|}
			\toprule
			\textbf{Bands} & \textbf{Wavelength (micrometers)} & \textbf{Resolution (meters)} \\ [0.2ex]
			\midrule
			\midrule
			Band 1 (Coastal aerosol) & 0.43-0.45 & 30 \\ [0.2ex]
			\midrule
			Band 2 (Blue) & 0.45-0.51 & 30 \\ [0.2ex]
			\midrule
			Band 3 (Green) & 0.53-0.59 & 30 \\ [0.2ex]
			\midrule
			Band 4 (Red) & 0.64-0.67 & 30 \\ [0.2ex]
			\hline
			Band 5 (Near Infrared) & 0.85-0.88 & 30 \\ [0.2ex]
			\midrule
			Band 6 (SWIR1) & 1.57-1.65 & 30 \\ [0.2ex]
			\midrule
			Band 7 (SWIR2) & 2.11-2.29 & 30 \\ [0.2ex]
			\midrule
			Band 8 (Panchromatic) & 0.50-0.68 & 15 \\ [0.2ex]
			\midrule
			Band 9 (Cirrus) & 1.36-1.38 & 30 \\ [0.2ex]
			\midrule
			Band 10 (TIR 1) & 10.6-11.19 & 100  \\ [0.2ex]
			\midrule
			Band 11 (TIR 2) & 11.50-12.51 & 100 \\ [0.2ex]
			\midrule
			\bottomrule
		\end{tabularx}
		\caption{Landsat 8-9 Operational Land Imager (OLI) and Thermal Infrared Sensor (TIRS) bands \cite{bands}.}
		\label{table:bands_table}
	\end{table}
	
	\subsection{World Glacier Inventory}
	\label{seq:wgi}
	The World Glacier Inventory (WGI) proves to be a useful resource for building our dataset, since it contains information for over 130,000 glaciers. Inventory parameters include geographic location, area, length, orientation, elevation, and classification. The WGI is based primarily on aerial photographs and maps with most glaciers having one data entry only. Hence, the data set can be viewed as a snapshot of the glacier distribution in the second half of the 20th century. It is based on the original WGI (WGMS 1989) from the World Glacier Monitoring Service \cite{WGI}.  
	There are a number of ways to retrieve data from the inventory:
	\begin{itemize}
		\item download the entire database in a single ASCII text file (wgi\_feb2012.csv);
		\item search by parameter using the Search Inventory interface;
		\item extract regions through the Extract Selected Regions interface.
	\end{itemize}

	The ASCII text file will be used with the purpose to define which are the glaciers to be included in the dataset to be built. An example of how this file looks like can be found in Figure~\ref{fig:WGI_ASCII}.
	
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.5]{../images/wgi_ASCII_file.png}
		\caption{World glacier inventory ASCII text file, as CSV}
		\label{fig:WGI_ASCII}
	\end{figure}
	
	The \emph{parameters} which will be extracted for the dataset construction are the following:
	\begin{itemize}
		\item \textbf{wgi\_glacier\_id}: unique id representing one glacier (or part of it, if the coverage area is larger);
		\item \textbf{glacier\_name}: name of the glacier (if it has one);
		\item \textbf{lat}: latitude of the glacier;
		\item \textbf{lon}: longitude of the glacier.
	\end{itemize}

	\subsection{Asset Acquisition}	
	
	Since Landsat 8 acquires over 700 scenes per day, this means that there are over two million scenes available for download, either making use of already built user friendly tools or by simply querying for them directly.
	
	\subsubsection{USGS Earth Explorer}
	
	One of the most popular services for satellite imagery downloading is USGS Earth Explorer. This is used for querying and ordering of satellite images, aerial photographs, and cartographic products through the U.S. Geological Survey. The tool is particularly useful when the main focus is to analyse a specific area rather than trying to acquire a large dataset of scenes. One can easily search for assets based on criteria such as world reference system path and row variables, latitude, longitude, cloud coverage, capture date and many others \cite{USGS}.
	
	However, downloading a large set of assets proves to be rather difficult by using this tool alone, since the parameters for each scene need to be manually set. On top of this, the query results have to be picked by hand and then passed for downloading through another application which handles their bulk download. This makes the process of building the dataset rather slow, frustrating and error prone. Such an example can be viewed in Figure~\ref{fig:EarthExplorer}, for the Belvedere glacier (45.942, 7.908).
	
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.25]{../images/EarthExplorer.png}
		\caption{EarthExplorer}
		\label{fig:EarthExplorer}
	\end{figure}
	
	\subsubsection{SpatioTemporal Asset Catalog API}
	\label{seq:STAC}
	
	In order to fix the problem of excessive manual labour which appeared by using the USGS Earth Explorer, we rather implemented an endpoint of the SpatioTemporal Asset Catalog API, specifically, the following: \textbf{\url{http://nsidc.org/data/glacier_inventory/index.html}} \cite{STAC}. The main idea of searching by using parameters still remains, but instead of manual inputting data for the search data, we rely on using the above-mentioned World Glacier Inventory ASCII text file, since it already has all the required information for each glacier.
	
	By using this method we can pick which glaciers we want to download based on their coordinates and calculate a bounding box representing the area we want to search, required for the STAC API query. Since there might be clouds which could obfuscate the area of interest in the image, we also add a maximum allowed cloud coverage along the bounding box.
	
	The STAC API query also requires a name for the collection of assets we want our queries to be made on, which for us is landsat-8-l1 (Landsat 8 Collection 1, Level 1). Using these three parameters we can now easily acquire a large number of assets with minimal manual labour, as compared to the more user friendly tool provided by USGS.
	
	The downloaded assets will be stored at a user specified disk location and they will be structured as shown in the Figure~\ref{fig:DownloadDirectory}.
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.4]{../images/DownloadDirectory.png}
		\caption{Download Directory}
		\label{fig:DownloadDirectory}
	\end{figure}
	
	\subsubsection{Landsat 8 Collection 1, Tier 1}
	
	To support analysis  of the Landsat long-term data record, the Landsat data archive was reorganized  into a formal tiered data collection  structure, which ensures that all Landsat Level 1 products provide a consistent archive of data quality to support time-series analysis and data “stacking”, while controlling continuous improvement of the archive, and access to all data as they are acquired. The implementation of collections ensures consistent and known radiometric and geometric quality through time and across instruments while improving control in the calibration and processing parameters \cite{lc1l1}. By using data from this collection, we ensure that our images are fit for accurate pixel-to-pixel processing.
	
	\section{Dataset entities}
	
	TODO better make this intro...
	We will further describe which are the bands necessary for the image generation and what are their uses. On top of this, we will further explain what is the normalized snow difference index and what is the optical flow used for.
	
	\subsection{Bands}
	
	Each Landsat 8 band is represented by a 16 bit grayscale image with a resolution between 7000 and 10000 pixels, each pixel representing 30 meters. We can conclude, therefore, that one scene covers around 200 and 300 km of Earth. Only the green and SWIR1 bands will be used for the purpose of this thesis and below we will discuss the specifications of each. 
	
	\subsubsection{Band 3 - Green Band}
	
	\begin{table} [h]
		\center
		\begin{tabular} {| l | l |}
			\hline
			\textbf{Wavelength} & {0.53- 0.59 micrometers} \\ [0.2ex]
			\hline
			\textbf{Spacial resolution} & {30 meters} \\ [0.2ex]
			\hline
			\textbf{Resolution} & {between 7000x7000 pixels and 10000x10000 pixels} \\ [0.2ex]
			\hline
			\textbf{Depth} & {16-bit}\\ [0.2ex]
			\hline
			\textbf{Format} & {grayscale}\\ [0.2ex]
			\hline
		\end{tabular}
		\caption{Landsat 8 green band specifications.}
		\label{table:green_table}
	\end{table}
	The green band, alongside with the red and blue ones, fall in the visible spectrum and it is usually used for mapping peak vegetation. Figure~\ref{fig:belvedere_green} is an example of the green band for the Belvedere glacier, specifically taken from scene LC81950282015098LGN01.
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.3]{../images/LC81950282015098LGN01_B3.png}
		\caption{Green band of scene LC81950282015098LGN01 from the Belvedere glacier, Italy.}
		\label{fig:belvedere_green}
	\end{figure}
	
	\subsubsection{Band 6 - SWIR1 Band}
	
	\begin{table} [h]
		\center
		\begin{tabular} {| l | l |}
			\hline
			\textbf{Wavelength} & {1.57 - 1.65 micrometers} \\ [0.2ex]
			\hline
			\textbf{Spacial resolution} & {30 meters} \\ [0.2ex]
			\hline
			\textbf{Resolution} & {between 7000x7000 pixels and 10000x10000 pixels} \\ [0.2ex]
			\hline
			\textbf{Depth} & {16-bit}\\ [0.2ex]
			\hline
			\textbf{Format} & {grayscale}\\ [0.2ex]
			\hline
		\end{tabular}
		\caption{Landsat 8 SWIR1 band specifications.}
		\label{table:swir1_table}
	\end{table}
	The shortwave infrared 1 band is particularly useful for enhancing geology objects such as rocks and soils which look similar in other bands \cite{bd}. Alongside this, it also discriminates moisture content of soil and vegetation and penetrates thin clouds \cite{bands}. Figure~\ref{fig:belvedere_swir1} is illustrated below as an example of a SWIR1 band taken from the same scene as the green one above.
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.3]{../images/LC81950282015098LGN01_B6.png}
		\caption{SWIR1 band of scene LC81950282015098LGN01 from the Belvedere glacier, Italy.}
		\label{fig:belvedere_swir1}
	\end{figure}
	
	
	\subsubsection{Normalized Snow Difference Index}
	\label{seq:ndsi_functional}
	
	The normalized snow difference index (NDSI) is an index which relates to the presence of snow/ice in a pixel. Snow and ice usually have a very high reflectance in the visible spectrum and very low one in the shortwave infrared one, which is useful for mapping out most types of clouds from the scene \cite{ndsi}. We can therefore use the formula from Equation~\ref{eq:ndsi_formula} in order to highlight the snow and ice pixels from a Landsat 8 image.
	\begin{equation}\label{eq:ndsi_formula}
		NDSI = \frac{green - SWIR1}{green + SWIR1}
	\end{equation}
	By combining the snow reflectance behaviour of snow and ice for each of these bands, we can create a normalized snow difference index image which has values in the range [-1, 1]. We can then apply a threshold on the pixels which states that if the NDSI value for a pixel is larger than 0.0, then that pixel represents snow/ice covered land; similarly, if its value is smaller than 0.0, that pixel represents snow/ice free land \cite{ndsi}, \cite{viirs}, as represented in Equation~\ref{eq:ndsi_threshold}. Of course, this represents the general value for the threshold, but with the increase of its value, we can more accurately differentiate between snow and ice. With larger threshold values we can state that a pixel represents ice covered land rather than just snow fall land \cite{viirs}. We have tried different values for the threshold and came to the conclusion that a value of 0.3 is best suited for our needs.
	\begin{equation}
		\begin{cases}\label{eq:ndsi_threshold}
			\text{snow/ice land} & \text{if } NDSI \geq 0.0\\
			\text{snow-free land}, & \text{NDSI < 0.0}
		\end{cases}
	\end{equation}
	An example of a generated NDSI for the Belvedere glacier, scene LC81940282013341LGN01, can be viewed in Figure~\ref{fig:belvedere_ndsi}.
	\begin{figure}[h]
	\centering
	\begin{minipage}{0.445\textwidth}
		\centering
		\includegraphics[width=\linewidth]{../images/LC81940282013341LGN01_NDSI_scaled.png}
	\end{minipage}
	\begin{minipage}{0.50\textwidth}
		\centering
		\includegraphics[width=\linewidth]{../images/LC81940282013341LGN01_NDSI_small.png}
	\end{minipage}
	\caption{NDSI image of scene LC81940282013341LGN01.}
	\label{fig:belvedere_ndsi}
	\end{figure}

	% ==================================================================================

	\section{Alignment}
	\label{seq:alignment_functional}
	Landsat's trajectory orbiting Earth is not fully precise, therefore not all images will be pixel-to-pixel aligned, which is a problem for image processing. Since we want to track each pixel's movement, we must be sure that its coordinates in the image do not change between any two given scenes. Figure~\ref{fig:unaligned} highlights the misalignment from scene between scenes LC81950282013316 and LC81950282013364, as an example.
	\begin{figure}[h]
	\centering
	\begin{minipage}{0.36\textwidth}
		\centering
		\includegraphics[width=\linewidth]{../images/unaligned.png}
	\end{minipage}
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\linewidth]{../images/unaligned_part.png}
	\end{minipage}
	\caption{Overlapped unaligned green band of scene LC81950282013316 and reference LC81950282013364 of the Belvedere glacier (195/028 WRS-2).}
	\label{fig:unaligned}	
	\end{figure}
	Since the bands we work with have a spacial resolution of 30 meters, which means that each pixel in the image represents 30 meters. Even with a misalignment of just 50 pixels we would end up with a 1.5 km difference between two scenes. Tracking pixels through the image without aligning them first would mean that we could never be sure that what we are looking at is indeed the same location. We have used different approaches on alignment which will be described in Section~\ref{seq:alignment_implementation}. Figure~\ref{fig:aligned} highlights the alignment corrected scene from Figure~\ref{fig:unaligned}. 
	\begin{figure}[h]
		\centering
		\begin{minipage}{0.36\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../images/aligned.png}
		\end{minipage}
		\begin{minipage}{0.45\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../images/aligned_part.png}
		\end{minipage}
		\caption{Overlapped aligned green band of scene LC81950282013316 and reference LC81950282013364 of the Belvedere glacier (195/028 WRS-2).}
		\label{fig:aligned}
	\end{figure}

	\subsubsection{NDSI's Optical Flow}
	\label{seq:ndsi_motion_matrix}
	Since our dataset represents a \textbf{time series of satellite images}, as described in Section~\ref{seq:landsat8_section}, we thought that in order to generate a new image of this series, we could extract the \textbf{motion of each pixel} from one scene to another. This information could then be applied between any two consecutive (date-wise) scenes from the set and we could update a pixel's coordinates based on the value its motion vector and store it in a new image.
	Extracting the motion vectors between two consecutive frames can be achieved by calculating their optical flow. \textbf{Optical flow} is defined as the motion of objects between consecutive frames of sequence, produced by the relative movement between the object and camera. By using computer vision algorithms which calculate the optical flow of two scenes, we could track the motion of melting glaciers across them in order to estimate their current velocity and possibly \textbf{predict their position} in the next frames \cite{opticalflow}.
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.6]{../images/optical_flow_schema.png}
		\caption{Optical flow problem visualisation.}
		\label{fig:optical_flow_example}
	\end{figure}
	
	Figure~\ref{fig:optical_flow_example} emphasizes the problem visually, where we can express an image as a \textbf{function of space}, with the coordinates \textbf{\((x, y)\)}, and \textbf{time} \textbf{\(t\)}. If we take the first image\textbf{ \(I(x, y, t)\)} and we move its pixels by a distance of \textbf{\(dx, dy\)} over a timestamp \textbf{\(dt\)}, we obtain the new image as follows: \textbf{\(I(x + dx, y + dy, t + dt)\)} \cite{orb}.
	
	There are multiple types of optical flow algorithms, but for the purpose of our thesis, we have chosen the \textbf{dense optical flow} one, specifically \textbf{Gunnar Farneback's}. Even if dense implementations have higher cost we chose to make this trade mainly because it calculates the motion for each pixel of the frame and it also has a higher accuracy \cite{orb} compared to methods such as Lucas-Kanade (sparse) \cite{lukas}.
	
	We have used optical flow as a tool to generate \textbf{distance vectors} based on motion between the NDSI(\(time=t\)) and NDSI(\(time=t + dt\)). These vectors will be used in order to create the \textbf{motion predicted NDSI}.
	
	\begin{figure}[h]
		\centering
		\begin{minipage}{0.45\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../images/LC81940282013341LGN01_Motion Vectros_scaled.png}
		\end{minipage}
		\begin{minipage}{0.50\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../images/LC81940282013341LGN01_Motion Vectros_small.png}
		\end{minipage}
		\caption{Optical flow vectors, where the previous image is LC81940282013341LGN01 and the current image is LC81940282013341LGN01.}
		\label{fig:aligned}
	\end{figure}

	\subsubsection{Motion Predicted NDSI}
	\label{seq:motion_ndsi_functional}
	
	Whilst optical flow allows us to see movement of the ice front which already happened in the past, we wanted to use this information and apply it on the most recent images such that we can estimate further glacier movements.

	We obtain the motion predicted generated NDSI by relocating each pixel value from the NDSI(\(time=t + dt\)) to a \textbf{predicted location}. For now, this location is generated by adding the distance vectors obtained by optical flow as described in Section~\ref{seq:motion_matrix_implementation} to the pixels of the same image, therefore shifting them by twice as much as they originally did, in their respective directions, as it can be observed in Figure~\ref{fig:motion_generated_schema}. More methods of calculating the predicted location can be found in Section~\ref{seq:future_development}.
	
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.5]{../images/motion_generated_schema.png}
		\caption{Motion generated NDSI algorithm.}
		\label{fig:motion_generated_schema}
	\end{figure}

	By applying this function to each pixel of the NDSI (\(t + dt\)) image and by making some adjustments which are further described in Section~\ref{seq:motion_ndsi_implementation}, for the Belvedere glacier, scene LC81940282015363LGN02, we have generated the motion predicted NDSI as shown in Figure~\ref{fig:motion_generated_belvedere}. The ice coverage for the generated image is 5.0991\% while the one for the actual NDSI(\(time=t+dt\)) is 5.3066\%. 

	\begin{figure}[h!]
	\centering
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\linewidth]{../images/LC81940282015363LGN02_NDSI_generated_scaled.png}
	\end{minipage}
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\linewidth]{../images/motion_predicted_2015_small.png}
	\end{minipage}
	\caption{}
	\label{fig:motion_generated_belvedere}
	\end{figure}
	
	Figure~\ref{fig:overlay} highlights the difference between the NDSI of the previous scene and the generated motion NDSI.
	
\begin{figure}[h!]
	\centering
		\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\linewidth]{../images/LC81940282015363LGN02_NDSI_generated_overlay_scaled.png}
		\end{minipage}
		\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\linewidth]{../images/motion_predicted_overlay_2015_small.png}
		\end{minipage}
		\caption{}
		\label{fig:overlay}
\end{figure}

	% =========================================================================	
	
	\section{Graphical User Interface}
	\label{seq:gui}

	\chapter{Design and Implementation}
	\label{cha:design_and_implementation}
	
	The design of the application will be discussed while focusing on two different aspects:
	\begin{itemize}
		\item \textbf{search and download}
		\item \textbf{processing}
	\end{itemize}
	The first section is optional and can be run independently from the processing, since the user can have an already created database of images. However, we have focused on simplifying the process of satellite imagery downloading as described in \ref{seq:STAC} through using the sat-search library as a helper for searching and downloading assets. More information on querying can be found in \ref{seq:sd_implementation}. 
	Given an existing set of data, the next step is to pass the root folder which contains the glacier directories to the processing unit, also designed as a plug-in mechanism which will trigger the graphical user interface to pop up and allow for the processing options to appear. More information on this section can be found at \ref{seq:processing}. From there, one can start processing by simply making use of the predefined buttons described in \ref{seq:gui}.
	
	\section{Search and Download}
	
	Searching as well as downloading have been implemented on top of the sat-search library and it is used directly from the command line interface by running the script described in Section~\ref{seq:search_download}.
	As an input we will be using a CSV file which will have the form as described in Section~\ref{seq:wgi}. Mainly we will need just four attributes to be specified for each desired glacier in order to create a query for searching, as follows: \textit{wgi\_glacier\_id, glacier\_name, lat and lon}.
	The CSV file will be intercepted by the \textbf{Download } class and sent for processing row by row (glacier by glacier) to the \textbf{GlacierFactory} class. This one is responsible for parsing each row of the input CSV file and transforming the information in \textbf{Glacier} objects, which will be passed back to the \textbf{Download} class.
	Using the newly created Glacier object we can construct its \textbf{search query} by calculating its bounding box, specifying the asset collection from which we request data and setting other parameters (maximum allowed cloud coverage, in our case). Listing~\ref{lst:query} represents an example of querying for glacier Belvedere, with the following parameters set in the CSV file: \textit{"IT4L01211009","BELVEDERE","45.942","7.908"}.
	\begin{lstlisting}[caption={Search query created by sat-search},label={lst:query},language=Bash]
		{"page": 1, "limit": 170, "bbox": [7.907990000000001, 45.94199, 7.90801, 45.94201], "query": {"eo:cloud_cover": {"lt": 10}}, "collection": "landsat-8-l1"}
	\end{lstlisting}
	The result of each glacier query will be automatically saved in a \textbf{JSON file} which is used as a data buffer between the search and download. The downloader takes each JSON file generated by the search engine and sends the command for getting that asset.
	The technical specifications of the Download, GlacierFactory and Glacier classes can be found in Figure~\ref{fig:sd_diagram}.
	\label{seq:sd_implementation}
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.23]{../images/sd_diagram.png}
		\caption{Technical specifications of Download, GlacierFactory and Glacier classes.}
		\label{fig:sd_diagram}
	\end{figure}
	\paragraph{Data corruption verification}
	Both the searching and downloading functions are executed concurrently, since they are \textbf{time intensive} tasks. Each band has around 60 MB, which would make one scene approximately 360 MB. For the Belvedere glaciers, we found 78 scenes with a cloud coverage of 10\%. This means that the size of the entire glacier data will be around 28 GB. Given that one asset's size is quite large, it takes a lot of time to finish the download. In this time, even a small connection interruption might result in corrupted data files, which would interfere with processing. Therefore, we implemented an \textbf{extra security measure} in the sat-stac library which verifies whether a file with the same name already exists at the download location. If it does, its size will be compared to the size taken from the STAC-API servers. In case the two file sizes don't match, it means that the file got corrupted during download and it will be downloaded again. If the sizes match, the downloader will skip the file and continue when it finds one requested file which was not yet found on the disk, such that we do not end up unnecessarily overwriting the already existing files in case of failure.
	
	\section{Processing}
	\label{seq:processing}
	Processing can be viewed as four different entities, as following:
	\begin{itemize}
		\item Image alignment;
		\item Normalized snow difference index;
		\item NDSI's motion matrix;
		\item Motion generated NDSI.
	\end{itemize}
	Before generating any images, we first need to make sure that the pixels between any two scenes are not misaligned. The details of the alignment process are described in Section~\ref{seq:alignment_implementation}. After we ensure that out images are fit, we move on to creating the normalized snow difference index image in order to enhance the ice through a set threshold (see Section~\ref{seq:ndsi_implementation}). We then create the matrix of pixel motions calculated between two consecutive (date-wise) scenes, process described in Section~\ref{seq:motion_matrix_implementation}. Based on the information of where each pixel has moved between two consecutive images, we can then create our future NDSI image by applying the motion vectors on each pixel from a scene. More details on the design and implementation of this part can be found in Section~\ref{seq:motion_ndsi_implementation}.
	The normalized snow difference index image will be computed for each scene as described in Section~\ref{seq:ndsi_functional}.
	
	%========================================================================================
	
	\subsection{Alignment}
	\label{seq:alignment_implementation}\
	
	
	In order to keep a high level of abstraction, we have split each scene into aligned and unaligned: Scene and AlignedScene objects. These and their children are created when crawling through the glaciers directory, specifically for each region of interest. However, aligning all the images when crawling would result into a lot of idle time for the user when starting the application and it would not be overall feasible to do so. Therefore, a scene is aligned only when it is specifically being selected in the graphical user interface (or when another entity needs it, such as optical flow and image generation). By doing this, we ensure that there is no unnecessary extra waiting time for each scene that we have when powering up the interface.
	As for the actual alignment, we have used an algorithm which \textbf{collects features} from each band of a scene and tries to match them with a given reference one's features. The obtained \textbf{matches} can be then used to create an \textbf{affine transformation matrix} which specifies by how much did the current image \textbf{rotated and translated} in comparison to its reference. The affine transformation matrix will be then applied to the current image by a \textbf{warping} algorithm with the goal of ensuring as much as possible that there is no misalignment between any pixel of two different images from the same time series. Section~\ref{seq:alignment_algorithm} describes more technically how we built this and what were the computer vision algorithms used for that matter. Figure~\ref{fig:alignment_diagram} contains the technical details of each class which takes part into the alignment process.
	
	
	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.45]{../images/alignment_diagram.png}
		\caption{Technical diagram for the SceneInterface, AlignedScene, AlignedBand and Image classes.}
		\label{fig:alignment_diagram}
	\end{figure}
	\subsubsection{Alignment algorithms}
	\label{seq:alignment_algorithm}
	
	As we have seen in Section~\ref{seq:landsat8_section}, the Landsat 8 satellite does not have a perfect trajectory, which results into misaligned images. These changes are not obvious to the naked eye from scene to scene, but overlapping two scenes highlights this problem, as we can see in Figure~\ref{fig:unaligned}. Even so the scenes are almost similar, which means that aligning them does not prove to be very hard and it is done by using a strong \textbf{keypoint detection algorithm} which in our case detects edges formed by mountains and other geographical features present in the scenes. The \textbf{computer vision algorithms} used for this are \textbf{ORB (Oriented FAST and Rotated BRIEF)}, \textbf{Harris Corner Detector} and \textbf{RANSAC (Random Sample Consensus)} and they are applied on the raw 16bit grayscale image.
	
	\textbf{ORB} represents a fusion between the features from accelerated segment test (\textbf{FAST}) keypoint detector and binary robust independent elementary features (\textbf{BRIEF}) descriptor. The FAST detector finds keypoints in the image and uses Harris corner measure to select the a number of top points from the list (25\%, in our case) \cite{orb}. In order to use the detector we must first normalize and downsample the 16bit raw image to 8 bit, as it is required by ORB. Each keypoint is then represented by a circle of 16 pixels. The descriptors must then identify these keypoints and pair with each other.
	We compute for each scene its \textbf{keypoints and descriptors} taken from \textbf{each band}, in order to increase the precision of alignment later on. Also, since we are working with satellite imagery there might be cases when the algorithm only finds keypoints in a small part of the image, resulting in erroneous distortion. \textbf{Splitting} the image into multiple boxes and applying the orb detect algorithm on each separate part of the image proved to solve this problem and create much better results. 
	
	After ensuring that we have good enough keypoints we will be aligning each scene with its reference by simply comparing the keypoint-descriptor pairs between the two scenes and checking which are equal. If two pairs are found to be equal, it means that the algorithm has found the same feature in both images and can calculate by how much it \textbf{rotated and shifted}. However, not all of these matches represent good results in our case, since the trajectory Landsat 8 can vary. We have found that selecting only \textbf{5000 keypoints} in the \textbf{top 25\% matches} from the total yielded in the better results overall. We then set the \textbf{maximum allowed shifting euclidean distance} between any two pixels to be at most 200, so 6km on the map, therefore getting rid of outlier matches based on the distance.
	The \textbf{matches} from the reference and image to be aligned will be used alongside the \textbf{Random Sample Consensus (RANSAC)} algorithm in order to create the \textbf{affine transformation matrix} using the cv2 library. RANSAC, proposed by Fischler and Bolles \cite{ransac}, is a general parameter estimation approach designed to handle a large proportion of outliers in the input data \cite{ransac2}. After we have the transformation matrix, we can \textbf{warp} it on the image to be aligned in order to get the result. 
	%========================================================================================
	
	\subsection{Normalized Snow Difference Index}
	\label{seq:ndsi_implementation}
	
	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.6]{../images/ndsi_diagram.png}
		\caption{Technical diagram for the NDSI class.}
		\label{fig:ndsi_diagram}
	\end{figure}

	The NDSI is generated by using the Formula~\ref{eq:ndsi_formula} described in section \ref{seq:ndsi_functional} on the green and shortwave infrared bands from a specific scene. Each band pixel is converted to \textbf{float32} in order to increase the \textbf{precision} of calculation; as each band is read as a n-dimensional array we can use numpy's optimised processing for generating the NDSI index image, since it converts the data and runs on native code rather than going through Python's interpretor. We then \textbf{filter} the generated image such that everything which is snow-free land will be -1 (\textbf{black}), but this is only applied for better visual analysis. In the background, we work with the full range of the image for better precision, which is the raw image. Figure~\ref{fig:ndsi_diagram} holds the technical diagram for the NDSI.

	%========================================================================================

	\subsection{NDSI's Optical Flow}
	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.6]{../images/motion_vectors_diagram.png}
		\caption{Technical diagram for the MotionVectors class.}
		\label{fig:motion_vectors_diagram}
	\end{figure}
	As described in Section~\ref{seq:motion_ndsi_implementation}, in order to determine the motion vector for each pixel between two images, we have used \textbf{Gunnar Farneback's} algorithm for dense optical flow calculation, implemented by the \textbf{OpenCV} library. The MotionVectors class takes as input two NDSI images.
	
	Since the NDSI images have been aligned, depending on the affine transformation matrix, they will not overlap perfectly any more, resulting in artifacts at their borders. Applying optical flow by using two NDSI images as such would result in highly distorted motion vectors at the borders, as it can be seen in Figure~\ref{fig:motion_vectors_error}. For fixing this, we have created a mask from each image such that both of them are cropped with the mask of the other. This results in losing a small number of pixels at the borders which could not be taken into consideration anyway.
	
	TODO GIVE EXAMPLE OF OPTICAL FLOW IMAGE
	
	In addition to the two NDSI image inputs, we have used the optical flow algorithm with the parameters specified in Listing~\ref{lst:opticalparameters}. We have used a pyramid scale of 0.5 and 6 pyramid levels having in mind that the images that we work with are large. By choosing this combination of parameters, we state that at each level, the image is going to be reshaped at half the size of the previous one; therefore the area of search for motion is small enough such that the optical flow is able to track movement.  

	\begin{lstlisting}[caption={Optical flow parameters},label={lst:opticalparameters},language=Python]
		cv2.calcOpticalFlowFarneback(..., pyr_scale=0.5, levels=6, winsize=15, iterations=3, poly_n=5, poly_sigma=1.2, flags=0)
	\end{lstlisting}

	After our images are perfectly overlaid, we can give them as inputs to the optical flow algorithm. The result of this will be the computed motions for each pixel, represented as a tuple of the distance that its coordinates moved from one frame to another. Specifically, as referring to Figure~\ref{fig:motion_generated_schema}, each item from the optical flow n-dimensional array will represent the motion distance vector (\(dx, dy\)) as calculated between NDSI(\(time=t\)) and NDSI(\(time=t + dt\)).
	
	To be able to visually interpret the optical flow output, we have used a colour coded image. Colour intensity is directly proportional to the motion vector length, while its hue represents the direction of this vector, as represented in Figure~\ref{fig:hue_colour_wheel}.
	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.3]{../images/colorwheel.png}
		\caption{Hue representation of the optical flow.}
		\label{fig:hue_colour_wheel}
	\end{figure}
	
	\begin{figure}[h!]
		\centering
		\begin{minipage}{0.45\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../images/LC81940282013341LGN01_Motion Vectros_color.png}
		\end{minipage}
		\begin{minipage}{0.45\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../images/LC81940282013341LGN01_Motion Vectros_color_small.png}
		\end{minipage}
		\caption{}
		\label{fig:NDSI}
	\end{figure}
	
	%========================================================================================

	\subsection{Motion Predicted NDSI}
	\label{seq:motion_ndsi_implementation}
	
	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.6]{../images/motion_created_diagram.png}
		\caption{Technical diagram for the MotionPredictedNDSI class.}
		\label{fig:motion_predicted_diagram}
	\end{figure}
	
	As means to put in practice what we described in Section~\ref{seq:motion_ndsi_functional}, we start by initialising a new numpy n-dimensional array based of the shape of the image NDSI(\(time=t+dt\)). The new array will be filled with the data generated by using the two required entities:
	\begin{itemize}
		\item \textbf{NDSI(\(time=t+dt\))};
		\item \textbf{motion vectors (\(dx, dy\))} extracted by optical flow.
	\end{itemize}
	
	Each motion vector (\(dx, dy\)) corresponds to the movement of a pixel from the NDSI(\(time=t+dt\)) image. We can use this information to generate the new predicted position (\(x + 2*dx, y + 2*dy\)) for each pixel of the NDSI(\(time=t+dt\)) image.
	
	
	\begin{algorithm}
			\SetKwInOut{Input}{Input}
			\SetKwInOut{Output}{Output}
			
			\underline{function generate} $(previous\_NDSI, motion\_vectors)$\;
			\Input{The NDSI(\(time=t+dt\) and the motion vectors generated by optical flow between NDSI(\(time=t\)) and NDSI(\(time=t+dt\))}
			\Output{The motion predicted NDSI(\(time=t+2*dt\)))}
			
			$motion\_predicted\_image \gets previous\_NDSI.shape$\;
			$width \gets NDSI.width()$\;
			$height \gets NDSI.height()$\;
			
			\For{$y$ in [$height, width$]}{
				\For{$x$ in [$height, width$]}{
					$dx \gets motion\_vectors[y][x][0]$\;
					$dy \gets motion\_vectors[y][x][1]$\;
	
					$new\_x \gets x + dx$\;
					$new\_y \gets y + dy$\;
					
					$motion\_predicted\_image[new\_y][new\_x] \gets previous\_NDSI[y][x]$\;
				} 
			}
			\Return{$motion\_predicted\_image$}\;

			\caption{Algorithm used for motion predicted image generation based on the optical flow vectors and NDSI(\(time=t+dt\))}
			\label{algo:change}
	\end{algorithm}
	
	As a first approach of populating the new image, we have simply iterated over all the pixels in the NDSI(\(time=t+dt\)) image and calculated their new coordinates based on their motion vector information, as it can be seen in Algorithm~\ref{algo:change}. Since this is an iterative approach, it does not scale for images as large as the ones we are using. On top of this, since we are using the Python language, which is an interpreted one, each operation has to go through an interpreter before being run. For very granular data processing this proves to be inefficient. For a typical image of 8543x8039 resolution, it takes as much as 10 minutes for generating the image on the machine that I have used (specifications at Section~\ref{seq:performance}).
	
	Since using the naive approach is not feasible in our application, we had to avoid iterating over the whole image in the first place. Therefore, instead of looping over each pixel in order to generate the values for the new image, we have made use of the numpy library to heavily optimise our data processing. By using numpy functions and vectorized operations directly instead of iterating we were able to improve the processing time by around 1700\%, bringing it down approximately from 10 minutes to 35 seconds.
	
	In the iterative approach, for each pixel we add its motion to its position such that we get its new location. These numbers can be computed ahead of time and transformed into arrays such that we only use numpy operations. By creating an array of the initial coordinates \(x, y\) and adding the motion vectors (\(dx, dy\)) array to it, we generated the absolute coordinates where each pixel from the NDSI image should be translated. By adding this modification we got rid of lines 5, 6, 7, 8, 9, 10 and 11 from the algorithm and replaced them with the code as it can be seen in Algorithm~\ref{algo:improved_generation}. The \(absolute\_coordinates\) and \(previous\_NDSI\) can be treated as a sparse array which is then densified using numpy capabilities.
	
	\begin{algorithm}
		\SetKwInOut{Input}{Input}
		\SetKwInOut{Output}{Output}
		
		$index\_array \gets [[[0, 0], [1, 0] ... [width, 0]$
						 $[0, 1], [1, 1] ... [width, 1]$
						 $...$
						 $[0, height], [1, height] ... [width, height]]]$\;
		$absolute\_coordinates \gets motion\_vectors + index\_array$\;
		
		$motion\_predicted\_image[absolute\_coordinates] \gets previous\_NDSI$\;
		
		\caption{Algorithm used for motion predicted image generation based on the optical flow vectors and NDSI(\(time=t+dt\))}
		\label{algo:improved_generation}
	\end{algorithm}
	
	The function used to generate the predicted NDSI does not have a one-to-one domain, thus making it undefined for certain inputs. This results in a very noisy generated image, as it can be seen in Figure TODO. We have implemented a filter which uses the weighted average of the neighbouring pixels values to fill the missing ones. The detailed description of the implementation can be found in Section~\ref{seq:filter}.
	
	\subsection{Generated image filtering}
	\label{seq:filter}
	
	\begin{figure}[h]
		\centering
		\begin{minipage}{0.45\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../images/LC81940282015363LGN02_Motion Predicted NDSI_unfiltered.png}
		\end{minipage}
		\begin{minipage}{0.45\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../images/LC81940282015363LGN02_Motion Predicted NDSI_unfiltered_small.png}
		\end{minipage}
		\caption{}
		\label{fig:unfiltered}
	\end{figure}
	
	A first step into creating the filter for the motion predicted NDSI image is to create a mask which will be applied on the black border of the scene such that we are looking for undefined values only inside it. Using numpy, the coordinates of these pixels are extracted into an array which then can be processed in parallel by Pythons' multiprocessing library. We have used multiprocessing instead of threading because Python does not have true parallelization in multithreading, only concurrency, which would not be a real improvement on the processing time. Since all the processes have to write different chunks of the same image and they do not share the same memory space, we chose to solve this problem by creating a shared memory buffer to hold the image.
	
	\begin{figure}[h!]
		\centering
		\begin{minipage}{0.44\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../images/gaussian_image.png}
		\end{minipage}
		\begin{minipage}{0.44\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../images/gaussian_kernel1.png}
		\end{minipage}
		\caption{Zoomed in part of the predicted NDSI (left). Neighbourhood extracted (right). }
		\label{fig:gaussian1}
	\end{figure}

	The value of each found undefined pixel has to be calculated as an weighted average composed of its neighbouring pixels, as shown in Figure~\ref{fig:gaussian1}. We created the kernel which holds the weight of each pixel in the neighbourhood such that pixels closer to the centre have a higher weights than those near the edge. However, we have the case when we have multiple undefined pixels in the same neighbourhood. Since we cannot take their values in consideration, we do not want to include them in the weighted average, thus we set their weight to be 0, as shown in Figure~\ref{fig:gaussian2} left. The result of the weighted average will be then stored as the value of the currently focused undefined pixel, as highlighted in Figure~\ref{fig:gaussian2} right.

	\begin{figure}[h!]
	\centering
	\begin{minipage}{0.44\textwidth}
		\centering
		\includegraphics[width=\linewidth]{../images/gaussian_weights.png}
	\end{minipage}
	\begin{minipage}{0.44\textwidth}
		\centering
		\includegraphics[width=\linewidth]{../images/gaussian_pixel.png}
	\end{minipage}
	\caption{Generated weights kernel (left). Filled in undefined value (right).}
	\label{fig:gaussian2}
	\end{figure}
	
	\begin{figure}[h!]
		\centering
		\begin{minipage}{0.45\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../images/LC81940282015363LGN02_NDSI_scaled.png}
		\end{minipage}
		\begin{minipage}{0.45\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../images/NDSI_2015_small.png}
		\end{minipage}
		\caption{}
		\label{fig:NDSI}
	\end{figure}
	
	\chapter{Performance}
	\label{seq:performance}
	Here we should also write on what machine I ran the code and try it on different ones as a thing. Also write about the idea of moving the processing on cloud, but there might be traffic overhead because the images are large. Also say that we store it on harddrive, NOT ssd. Make tests with both.
	
	\chapter{Conclusions}
	\section{Future Development}
	\label{seq:future_development}
	
	\bibliographystyle{alpha}
	\bibliography{references}
	

	\chapter{Glossary}
	
	% ==================================================================
	
	\section{Acronyms}
	
		\begin{table} [H]
		\centering
		\begin{tabular} {|  l | L{10cm} |}
			\hline
			WGI & World Glacier Inventory \\ [0.2ex]
			\hline
			NDSI & Normalized Snow Difference Index \\ [0.2ex]
			\hline
			\hline
			CSV & Comma separated values \\ [0.2ex]
			\hline
			\hline
			JSON & JavaScript Object Notation \\ [0.2ex]
			\hline
		\end{tabular}
		\caption{Acronyms table }
		\label{table:acron}
	\end{table}
	\end{document}

EXTRAS 
This part of the application can be viewed as a plug-in entity, since one can already have a predefined dataset which can be loaded directly to the processing unit.
